{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP & representation learning: Neural Embeddings, Text Classification\n",
    "\n",
    "## CELIK Simay 28713301 - SOYKOK Aylin 28711545\n",
    "\n",
    "To use statistical classifiers with text, it is first necessary to vectorize the text. In the first practical session we explored the **Bag of Word (BoW)** model. \n",
    "\n",
    "Modern **state of the art** methods uses  embeddings to vectorize the text before classification in order to avoid feature engineering.\n",
    "\n",
    "## [Dataset](https://thome.isir.upmc.fr/classes/RITAL/json_pol.json)\n",
    "\n",
    "\n",
    "## \"Modern\" NLP pipeline\n",
    "\n",
    "By opposition to the **bag of word** model, in the modern NLP pipeline everything is **embeddings**. Instead of encoding a text as a **sparse vector** of length $D$ (size of feature dictionnary) the goal is to encode the text in a meaningful dense vector of a small size $|e| <<< |D|$. \n",
    "\n",
    "\n",
    "The raw classification pipeline is then the following:\n",
    "\n",
    "```\n",
    "raw text ---|embedding table|-->  vectors --|Neural Net|--> class \n",
    "```\n",
    "\n",
    "\n",
    "### Using a  language model:\n",
    "\n",
    "How to tokenize the text and extract a feature dictionnary is still a manual task. To directly have meaningful embeddings, it is common to use a pre-trained language model such as `word2vec` which we explore in this practical.\n",
    "\n",
    "In this setting, the pipeline becomes the following:\n",
    "```\n",
    "      \n",
    "raw text ---|(pre-trained) Language Model|--> vectors --|classifier (or fine-tuning)|--> class \n",
    "```\n",
    "\n",
    "\n",
    "- #### Classic word embeddings\n",
    "\n",
    " - [Word2Vec](https://arxiv.org/abs/1301.3781)\n",
    " - [Glove](https://nlp.stanford.edu/projects/glove/)\n",
    "\n",
    "\n",
    "- #### bleeding edge language models techniques (see next)\n",
    "\n",
    " - [UMLFIT](https://arxiv.org/abs/1801.06146)\n",
    " - [ELMO](https://arxiv.org/abs/1802.05365)\n",
    " - [GPT](https://blog.openai.com/language-unsupervised/)\n",
    " - [BERT](https://arxiv.org/abs/1810.04805)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Goal of this session:\n",
    "\n",
    "1. Train word embeddings on training dataset\n",
    "2. Tinker with the learnt embeddings and see learnt relations\n",
    "3. Tinker with pre-trained embeddings.\n",
    "4. Use those embeddings for classification\n",
    "5. Compare different embedding models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 0: Loading data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reviews :  25000\n",
      "----> # of positive :  12500\n",
      "----> # of negative :  12500\n",
      "\n",
      "['Although credit should have been given to Dr. Seuess for stealing the story-line of \"Horton Hatches The Egg\", this was a fine film. It touched both the emotions and the intellect. Due especially to the incredible performance of seven year old Justin Henry and a script that was sympathetic to each character (and each one\\'s predicament), the thought provoking elements linger long after the tear jerking ones are over. Overall, superior acting from a solid cast, excellent directing, and a very powerful script. The right touches of humor throughout help keep a \"heavy\" subject from becoming tedious or difficult to sit through. Lastly, this film stands the test of time and seems in no way dated, decades after it was released.', 1]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "# Loading json\n",
    "file = './datasets/json_pol.json'\n",
    "with open(file,encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "\n",
    "# Quick Check\n",
    "counter = Counter((x[1] for x in data))\n",
    "print(\"Number of reviews : \", len(data))\n",
    "print(\"----> # of positive : \", counter[1])\n",
    "print(\"----> # of negative : \", counter[0])\n",
    "print(\"\")\n",
    "print(data[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing\n",
    "import string\n",
    "import re\n",
    "punc = string.punctuation+'\\n\\r\\t\"'\n",
    "def preprocess(text):\n",
    "    \"\"\"Suppressing numbers, lowering strings and removing punctuations\"\"\"\n",
    "    chiffsupp = re.sub('[0-9]+', '', text)\n",
    "    return chiffsupp.translate(str.maketrans(punc, ' ' * len(punc))).lower()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec: Quick Recap\n",
    "\n",
    "**[Word2Vec](https://arxiv.org/abs/1301.3781) is composed of two distinct language models (CBOW and SG), optimized to quickly learn word vectors**\n",
    "\n",
    "\n",
    "given a random text: `i'm taking the dog out for a walk`\n",
    "\n",
    "\n",
    "\n",
    "### (a) Continuous Bag of Word (CBOW)\n",
    "    -  predicts a word given a context\n",
    "    \n",
    "maximizing `p(dog | i'm taking the ___ out for a walk)`\n",
    "    \n",
    "### (b) Skip-Gram (SG)               \n",
    "    -  predicts a context given a word\n",
    "    \n",
    " maximizing `p(i'm taking the out for a walk | dog)`\n",
    "\n",
    "\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 1: train a language model (word2vec)\n",
    "\n",
    "Gensim has one of [Word2Vec](https://radimrehurek.com/gensim/models/word2vec.html) fastest implementation.\n",
    "\n",
    "\n",
    "### Train:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if gensim not installed yet\n",
    "# ! pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-27 19:31:20,913 : INFO : collecting all words and their counts\n",
      "2024-02-27 19:31:20,913 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2024-02-27 19:31:21,308 : INFO : PROGRESS: at sentence #10000, processed 2301366 words, keeping 153853 word types\n",
      "2024-02-27 19:31:21,706 : INFO : PROGRESS: at sentence #20000, processed 4553558 words, keeping 240043 word types\n",
      "2024-02-27 19:31:21,927 : INFO : collected 276678 word types from a corpus of 5713167 raw words and 25000 sentences\n",
      "2024-02-27 19:31:21,928 : INFO : Creating a fresh vocabulary\n",
      "2024-02-27 19:31:22,125 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 48208 unique words (17.42% of original 276678, drops 228470)', 'datetime': '2024-02-27T19:31:22.125328', 'gensim': '4.3.0', 'python': '3.8.17 (default, Jul  5 2023, 16:07:30) \\n[Clang 14.0.6 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "2024-02-27 19:31:22,126 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 5389596 word corpus (94.34% of original 5713167, drops 323571)', 'datetime': '2024-02-27T19:31:22.126183', 'gensim': '4.3.0', 'python': '3.8.17 (default, Jul  5 2023, 16:07:30) \\n[Clang 14.0.6 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "2024-02-27 19:31:22,348 : INFO : deleting the raw counts dictionary of 276678 items\n",
      "2024-02-27 19:31:22,352 : INFO : sample=0.001 downsamples 44 most-common words\n",
      "2024-02-27 19:31:22,353 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 4165010.0598832574 word corpus (77.3%% of prior 5389596)', 'datetime': '2024-02-27T19:31:22.353299', 'gensim': '4.3.0', 'python': '3.8.17 (default, Jul  5 2023, 16:07:30) \\n[Clang 14.0.6 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "2024-02-27 19:31:22,724 : INFO : estimated required memory for 48208 words and 100 dimensions: 62670400 bytes\n",
      "2024-02-27 19:31:22,725 : INFO : resetting layer weights\n",
      "2024-02-27 19:31:22,764 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-02-27T19:31:22.764166', 'gensim': '4.3.0', 'python': '3.8.17 (default, Jul  5 2023, 16:07:30) \\n[Clang 14.0.6 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'build_vocab'}\n",
      "2024-02-27 19:31:22,765 : INFO : Word2Vec lifecycle event {'msg': 'training model with 3 workers on 48208 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2024-02-27T19:31:22.764988', 'gensim': '4.3.0', 'python': '3.8.17 (default, Jul  5 2023, 16:07:30) \\n[Clang 14.0.6 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'train'}\n",
      "2024-02-27 19:31:23,781 : INFO : EPOCH 0 - PROGRESS: at 8.87% examples, 365969 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 19:31:24,789 : INFO : EPOCH 0 - PROGRESS: at 18.19% examples, 373874 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 19:31:25,799 : INFO : EPOCH 0 - PROGRESS: at 27.52% examples, 378802 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 19:31:26,805 : INFO : EPOCH 0 - PROGRESS: at 36.70% examples, 379855 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 19:31:27,825 : INFO : EPOCH 0 - PROGRESS: at 46.10% examples, 379769 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 19:31:28,830 : INFO : EPOCH 0 - PROGRESS: at 55.55% examples, 380232 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 19:31:29,847 : INFO : EPOCH 0 - PROGRESS: at 64.74% examples, 380496 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 19:31:30,881 : INFO : EPOCH 0 - PROGRESS: at 74.16% examples, 380052 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 19:31:31,887 : INFO : EPOCH 0 - PROGRESS: at 83.18% examples, 378441 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 19:31:32,899 : INFO : EPOCH 0 - PROGRESS: at 92.32% examples, 378903 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 19:31:33,737 : INFO : EPOCH 0: training on 5713167 raw words (4164780 effective words) took 11.0s, 379653 effective words/s\n",
      "2024-02-27 19:31:34,750 : INFO : EPOCH 1 - PROGRESS: at 9.02% examples, 374469 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 19:31:35,774 : INFO : EPOCH 1 - PROGRESS: at 19.18% examples, 392581 words/s, in_qsize 6, out_qsize 0\n",
      "2024-02-27 19:31:36,801 : INFO : EPOCH 1 - PROGRESS: at 28.72% examples, 391423 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 19:31:37,809 : INFO : EPOCH 1 - PROGRESS: at 37.89% examples, 389227 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 19:31:38,822 : INFO : EPOCH 1 - PROGRESS: at 47.34% examples, 387583 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 19:31:39,824 : INFO : EPOCH 1 - PROGRESS: at 56.76% examples, 387135 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 19:31:40,838 : INFO : EPOCH 1 - PROGRESS: at 65.89% examples, 386543 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 19:31:41,863 : INFO : EPOCH 1 - PROGRESS: at 75.50% examples, 385772 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 19:31:42,876 : INFO : EPOCH 1 - PROGRESS: at 84.83% examples, 385618 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 19:31:43,897 : INFO : EPOCH 1 - PROGRESS: at 94.21% examples, 385795 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 19:31:44,526 : INFO : EPOCH 1: training on 5713167 raw words (4164397 effective words) took 10.8s, 386077 effective words/s\n",
      "2024-02-27 19:31:45,554 : INFO : EPOCH 2 - PROGRESS: at 8.87% examples, 361461 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 19:31:46,577 : INFO : EPOCH 2 - PROGRESS: at 18.17% examples, 368851 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 19:31:47,598 : INFO : EPOCH 2 - PROGRESS: at 27.37% examples, 371483 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 19:31:48,618 : INFO : EPOCH 2 - PROGRESS: at 36.52% examples, 373088 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 19:31:49,629 : INFO : EPOCH 2 - PROGRESS: at 46.10% examples, 376304 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 19:31:50,650 : INFO : EPOCH 2 - PROGRESS: at 55.73% examples, 377602 words/s, in_qsize 6, out_qsize 0\n",
      "2024-02-27 19:31:51,651 : INFO : EPOCH 2 - PROGRESS: at 64.74% examples, 378089 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 19:31:52,667 : INFO : EPOCH 2 - PROGRESS: at 74.36% examples, 379721 words/s, in_qsize 6, out_qsize 0\n",
      "2024-02-27 19:31:53,677 : INFO : EPOCH 2 - PROGRESS: at 83.84% examples, 380342 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 19:31:54,703 : INFO : EPOCH 2 - PROGRESS: at 92.97% examples, 380106 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 19:31:55,458 : INFO : EPOCH 2: training on 5713167 raw words (4163538 effective words) took 10.9s, 380929 effective words/s\n",
      "2024-02-27 19:31:56,473 : INFO : EPOCH 3 - PROGRESS: at 8.87% examples, 366417 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 19:31:57,478 : INFO : EPOCH 3 - PROGRESS: at 18.19% examples, 374645 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 19:31:58,485 : INFO : EPOCH 3 - PROGRESS: at 27.17% examples, 374959 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 19:31:59,512 : INFO : EPOCH 3 - PROGRESS: at 36.38% examples, 375052 words/s, in_qsize 6, out_qsize 0\n",
      "2024-02-27 19:32:00,529 : INFO : EPOCH 3 - PROGRESS: at 46.10% examples, 378871 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 19:32:01,538 : INFO : EPOCH 3 - PROGRESS: at 55.55% examples, 379354 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 19:32:02,539 : INFO : EPOCH 3 - PROGRESS: at 64.55% examples, 379628 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 19:32:03,547 : INFO : EPOCH 3 - PROGRESS: at 73.99% examples, 380585 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 19:32:04,561 : INFO : EPOCH 3 - PROGRESS: at 83.52% examples, 380960 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 19:32:05,561 : INFO : EPOCH 3 - PROGRESS: at 92.82% examples, 382322 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 19:32:06,354 : INFO : EPOCH 3: training on 5713167 raw words (4165520 effective words) took 10.9s, 382395 effective words/s\n",
      "2024-02-27 19:32:07,371 : INFO : EPOCH 4 - PROGRESS: at 9.02% examples, 372588 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 19:32:08,385 : INFO : EPOCH 4 - PROGRESS: at 18.53% examples, 379657 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 19:32:09,402 : INFO : EPOCH 4 - PROGRESS: at 27.92% examples, 381640 words/s, in_qsize 6, out_qsize 0\n",
      "2024-02-27 19:32:10,409 : INFO : EPOCH 4 - PROGRESS: at 37.03% examples, 381958 words/s, in_qsize 5, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-27 19:32:11,435 : INFO : EPOCH 4 - PROGRESS: at 46.66% examples, 382353 words/s, in_qsize 6, out_qsize 0\n",
      "2024-02-27 19:32:12,450 : INFO : EPOCH 4 - PROGRESS: at 56.22% examples, 383019 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 19:32:13,451 : INFO : EPOCH 4 - PROGRESS: at 65.24% examples, 382783 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 19:32:14,453 : INFO : EPOCH 4 - PROGRESS: at 74.78% examples, 383631 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 19:32:15,463 : INFO : EPOCH 4 - PROGRESS: at 83.84% examples, 382238 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 19:32:16,469 : INFO : EPOCH 4 - PROGRESS: at 93.16% examples, 383294 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 19:32:17,216 : INFO : EPOCH 4: training on 5713167 raw words (4165478 effective words) took 10.9s, 383551 effective words/s\n",
      "2024-02-27 19:32:17,217 : INFO : Word2Vec lifecycle event {'msg': 'training on 28565835 raw words (20823713 effective words) took 54.5s, 382420 effective words/s', 'datetime': '2024-02-27T19:32:17.217381', 'gensim': '4.3.0', 'python': '3.8.17 (default, Jul  5 2023, 16:07:30) \\n[Clang 14.0.6 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'train'}\n",
      "2024-02-27 19:32:17,218 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=48208, vector_size=100, alpha=0.025>', 'datetime': '2024-02-27T19:32:17.218064', 'gensim': '4.3.0', 'python': '3.8.17 (default, Jul  5 2023, 16:07:30) \\n[Clang 14.0.6 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "text = [t.split() for t,p in data]\n",
    "\n",
    "# the following configuration is the default configuration\n",
    "w2v = gensim.models.word2vec.Word2Vec(sentences=text,\n",
    "                                vector_size=100, window=5,               ### here we train a cbow model \n",
    "                                min_count=5,                      \n",
    "                                sample=0.001, workers=3,\n",
    "                                sg=1, hs=0, negative=5,        ### set sg to 1 to train a sg model\n",
    "                                cbow_mean=1, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-27 20:01:12,112 : INFO : Word2Vec lifecycle event {'fname_or_handle': 'W2v-movies.dat', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2024-02-27T20:01:12.112532', 'gensim': '4.3.0', 'python': '3.8.17 (default, Jul  5 2023, 16:07:30) \\n[Clang 14.0.6 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'saving'}\n",
      "2024-02-27 20:01:12,117 : INFO : not storing attribute cum_table\n",
      "2024-02-27 20:01:12,195 : INFO : saved W2v-movies.dat\n"
     ]
    }
   ],
   "source": [
    "# Worth it to save the previous embedding\n",
    "w2v.save(\"W2v-movies.dat\")\n",
    "# You will be able to reload them:\n",
    "# w2v = gensim.models.Word2Vec.load(\"W2v-movies.dat\")\n",
    "# and you can continue the learning process if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words found: 48208\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(w2v.wv.key_to_index)\n",
    "print(\"Number of unique words found:\", vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 2: Test learnt embeddings\n",
    "\n",
    "The word embedding space directly encodes similarities between words: the vector coding for the word \"great\" will be closer to the vector coding for \"good\" than to the one coding for \"bad\". Generally, [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity) is the distance used when considering distance between vectors.\n",
    "\n",
    "KeyedVectors have a built in [similarity](https://radimrehurek.com/gensim/models /keyedvectors.html#gensim.models.keyedvectors.BaseKeyedVectors.similarity) method to compute the cosine similarity between words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "great and good: 0.7652735\n",
      "great and bad: 0.48330587\n"
     ]
    }
   ],
   "source": [
    "# is great really closer to good than to bad ?\n",
    "print(\"great and good:\",w2v.wv.similarity(\"great\",\"good\"))\n",
    "print(\"great and bad:\",w2v.wv.similarity(\"great\",\"bad\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since cosine distance encodes similarity, neighboring words are supposed to be similar. The [most_similar](https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.BaseKeyedVectors.most_similar) method returns the `topn` words given a query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('film', 0.9344167709350586), ('\"movie\"', 0.8251368999481201), ('flick', 0.7798967361450195), ('movie,', 0.7704945206642151), ('stinker', 0.729508638381958)]\n",
      "[('amazing', 0.7707414031028748), ('excellent', 0.7452518939971924), ('terrific', 0.685361385345459), ('fantastic', 0.6832655668258667), ('cool', 0.6712515950202942)]\n",
      "[('actor,', 0.8258581161499023), ('actor.', 0.74735027551651), ('actress', 0.7395841479301453), ('Reeves', 0.7224267721176147), ('role,', 0.7131319642066956)]\n"
     ]
    }
   ],
   "source": [
    "# The query can be as simple as a word, such as \"movie\"\n",
    "\n",
    "# Try changing the word\n",
    "print(w2v.wv.most_similar(\"movie\",topn=5)) # 5 most similar words\n",
    "print(w2v.wv.most_similar(\"awesome\",topn=5))\n",
    "print(w2v.wv.most_similar(\"actor\",topn=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But it can be a more complicated query\n",
    "Word embedding spaces tend to encode much more.\n",
    "\n",
    "The most famous exemple is: `vec(king) - vec(man) + vec(woman) => vec(queen)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('awful', 0.7509977221488953),\n",
       " ('horrible', 0.6313647031784058),\n",
       " ('atrocious', 0.6246147751808167)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What is awesome - good + bad ?\n",
    "w2v.wv.most_similar(positive=[\"awesome\",\"bad\"],negative=[\"good\"],topn=3)  \n",
    "\n",
    "\n",
    "# Try other things like plurals for exemple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('actress', 0.8075904250144958),\n",
       " ('actress,', 0.7300375699996948),\n",
       " ('role', 0.6861469149589539)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.wv.most_similar(positive=[\"actor\",\"woman\"],negative=[\"man\"],topn=3) # do the famous exemple works for actor ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('actresses', 0.7714454531669617),\n",
       " ('actors/actresses', 0.7179805636405945),\n",
       " ('actors,', 0.6822283864021301)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.wv.most_similar(positive=[\"actors\",\"women\"],negative=[\"men\"],topn=3) # \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To test learnt \"synctactic\" and \"semantic\" similarities, Mikolov et al. introduced a special dataset containing a wide variety of three way similarities.**\n",
    "\n",
    "**You can download the dataset [here](https://thome.isir.upmc.fr/classes/RITAL/questions-words.txt).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-27 20:26:51,060 : INFO : Evaluating word analogies for top 300000 words in the model on ressources/questions-words.txt\n",
      "2024-02-27 20:26:51,153 : INFO : capital-common-countries: 2.2% (2/90)\n",
      "2024-02-27 20:26:51,222 : INFO : capital-world: 1.4% (1/71)\n",
      "2024-02-27 20:26:51,250 : INFO : currency: 0.0% (0/28)\n",
      "2024-02-27 20:26:51,553 : INFO : city-in-state: 0.0% (0/329)\n",
      "2024-02-27 20:26:51,867 : INFO : family: 36.8% (126/342)\n",
      "2024-02-27 20:26:52,683 : INFO : gram1-adjective-to-adverb: 2.7% (25/930)\n",
      "2024-02-27 20:26:53,178 : INFO : gram2-opposite: 3.1% (17/552)\n",
      "2024-02-27 20:26:54,189 : INFO : gram3-comparative: 22.1% (278/1260)\n",
      "2024-02-27 20:26:54,735 : INFO : gram4-superlative: 5.6% (39/702)\n",
      "2024-02-27 20:26:55,396 : INFO : gram5-present-participle: 17.1% (129/756)\n",
      "2024-02-27 20:26:56,110 : INFO : gram6-nationality-adjective: 3.4% (27/792)\n",
      "2024-02-27 20:26:57,179 : INFO : gram7-past-tense: 16.5% (208/1260)\n",
      "2024-02-27 20:26:57,841 : INFO : gram8-plural: 6.7% (54/812)\n",
      "2024-02-27 20:26:58,525 : INFO : gram9-plural-verbs: 27.1% (205/756)\n",
      "2024-02-27 20:26:58,526 : INFO : Quadruplets with out-of-vocabulary words: 55.6%\n",
      "2024-02-27 20:26:58,526 : INFO : NB: analogies containing OOV words were skipped from evaluation! To change this behavior, use \"dummy4unknown=True\"\n",
      "2024-02-27 20:26:58,527 : INFO : Total accuracy: 12.8% (1111/8680)\n"
     ]
    }
   ],
   "source": [
    "out = w2v.wv.evaluate_word_analogies(\"ressources/questions-words.txt\",case_insensitive=True)  #original semantic syntactic dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**When training the w2v models on the review dataset, since it hasn't been learnt with a lot of data, it does not perform very well.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 3: Loading a pre-trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Gensim, embeddings are loaded and can be used via the [\"KeyedVectors\"](https://radimrehurek.com/gensim/models/keyedvectors.html) class\n",
    "\n",
    "> Since trained word vectors are independent from the way they were trained (Word2Vec, FastText, WordRank, VarEmbed etc), they can be represented by a standalone structure, as implemented in this module.\n",
    "\n",
    ">The structure is called “KeyedVectors” and is essentially a mapping between entities and vectors. Each entity is identified by its string id, so this is a mapping between {str => 1D numpy array}.\n",
    "\n",
    ">The entity typically corresponds to a word (so the mapping maps words to 1D vectors), but for some models, they key can also correspond to a document, a graph node etc. To generalize over different use-cases, this module calls the keys entities. Each entity is always represented by its string id, no matter whether the entity is a word, a document or a graph node.\n",
    "\n",
    "**You can download the pre-trained word embedding [HERE](https://thome.isir.upmc.fr/classes/RITAL/word2vec-google-news-300.dat) .**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-27 20:59:49,738 : INFO : loading KeyedVectors object from word2vec-google-news-300/word2vec-google-news-300.dat\n",
      "2024-02-27 20:59:50,902 : INFO : loading vectors from word2vec-google-news-300/word2vec-google-news-300.dat.vectors.npy with mmap=None\n",
      "2024-02-27 20:59:52,880 : INFO : KeyedVectors lifecycle event {'fname': 'word2vec-google-news-300/word2vec-google-news-300.dat', 'datetime': '2024-02-27T20:59:52.879874', 'gensim': '4.3.0', 'python': '3.8.17 (default, Jul  5 2023, 16:07:30) \\n[Clang 14.0.6 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'loaded'}\n"
     ]
    }
   ],
   "source": [
    "from gensim.test.utils import get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "import gensim.downloader as api\n",
    "bload = True\n",
    "fname = \"word2vec-google-news-300\"\n",
    "sdir = \"word2vec-google-news-300/\" # Change\n",
    "\n",
    "if(bload==True):\n",
    "    wv_pre_trained = KeyedVectors.load(sdir+fname+\".dat\")\n",
    "else:    \n",
    "    wv_pre_trained = api.load(fname)\n",
    "    wv_pre_trained.save(sdir+fname+\".dat\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Perform the \"synctactic\" and \"semantic\" evaluations again. Conclude on the pre-trained embeddings.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#out = wv_pre_trained.evaluate_word_analogies(\"ressources/questions-words_pretrained.txt\",case_insensitive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "great and good: 0.729151\n",
      "great and bad: 0.3928765\n"
     ]
    }
   ],
   "source": [
    "print(\"great and good:\",wv_pre_trained.similarity(\"great\",\"good\"))\n",
    "print(\"great and bad:\",wv_pre_trained.similarity(\"great\",\"bad\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "movie :\n",
      "\t ('film', 0.8676770329475403)\n",
      "\t ('movies', 0.8013108372688293)\n",
      "\t ('films', 0.7363012433052063)\n",
      "\t ('moive', 0.6830360889434814)\n",
      "\t ('Movie', 0.6693678498268127)\n",
      "awesome :\n",
      "\t ('amazing', 0.8282864689826965)\n",
      "\t ('unbelievable', 0.7464959025382996)\n",
      "\t ('fantastic', 0.7453291416168213)\n",
      "\t ('incredible', 0.7390913963317871)\n",
      "\t ('unbelieveable', 0.6678117513656616)\n",
      "actor :\n",
      "\t ('actress', 0.7930009961128235)\n",
      "\t ('Actor', 0.7446157336235046)\n",
      "\t ('thesp', 0.6954972147941589)\n",
      "\t ('thespian', 0.6651668548583984)\n",
      "\t ('actors', 0.6519852876663208)\n"
     ]
    }
   ],
   "source": [
    "for word in ['movie', 'awesome', 'actor']:\n",
    "    print(word, ':')\n",
    "    for item in wv_pre_trained.most_similar(word,topn=5):\n",
    "        print('\\t', item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "awesome bad -- good\n",
      "\t ('horrible', 0.5953485369682312)\n",
      "\t ('amazing', 0.5928210020065308)\n",
      "\t ('weird', 0.5782380700111389)\n",
      "\t ('freaky', 0.5767402648925781)\n",
      "\t ('unbelievable', 0.5747914910316467)\n",
      "actor woman -- man\n",
      "\t ('actress', 0.8602624535560608)\n",
      "\t ('actresses', 0.6596671342849731)\n",
      "\t ('thesp', 0.6290916800498962)\n",
      "\t ('Actress', 0.6165293455123901)\n",
      "\t ('actress_Rachel_Weisz', 0.5997322201728821)\n"
     ]
    }
   ],
   "source": [
    "positives = [\n",
    "    [\"awesome\",\"bad\"],\n",
    "    [\"actor\",\"woman\"]\n",
    "]\n",
    "\n",
    "negatives = [\n",
    "    [\"good\"],\n",
    "    [\"man\"]\n",
    "]\n",
    "\n",
    "for i in range(2):\n",
    "    print(*positives[i], '--', *negatives[i])\n",
    "    for item in wv_pre_trained.most_similar(positive=positives[i],negative=negatives[i],topn=5):\n",
    "        print('\\t', item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "movie movies -- film\n",
      "\t ('films', 0.6389263272285461)\n",
      "\t ('Movies', 0.6188488006591797)\n",
      "\t ('flicks', 0.6120561361312866)\n",
      "\t ('Hollywood_blockbusters', 0.5958892703056335)\n",
      "\t ('romcoms', 0.5864962339401245)\n",
      "actor actors -- actress\n",
      "\t ('Actors', 0.6128960251808167)\n",
      "\t ('thesps', 0.5747197270393372)\n",
      "\t ('screenwriters', 0.5588046312332153)\n",
      "\t ('thespians', 0.5531652569770813)\n",
      "\t ('thespian', 0.5476460456848145)\n"
     ]
    }
   ],
   "source": [
    "positives = [\n",
    "    [\"movie\",\"movies\"],\n",
    "    [\"actor\",\"actors\"]\n",
    "]\n",
    "\n",
    "negatives = [\n",
    "    [\"film\"],\n",
    "    [\"actress\"]\n",
    "]\n",
    "\n",
    "for i in range(2):\n",
    "    print(*positives[i], '--', *negatives[i])\n",
    "    for item in wv_pre_trained.most_similar(positive=positives[i],negative=negatives[i],topn=5):\n",
    "        print('\\t', item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 4:  sentiment classification\n",
    "\n",
    "In the previous practical session, we used a bag of word approach to transform text into vectors.\n",
    "Here, we propose to try to use word vectors (previously learnt or loaded).\n",
    "\n",
    "\n",
    "### <font color='green'> Since we have only word vectors and that sentences are made of multiple words, we need to aggregate them. </font>\n",
    "\n",
    "\n",
    "### (1) Vectorize reviews using word vectors:\n",
    "\n",
    "Word aggregation can be done in different ways:\n",
    "\n",
    "- Sum\n",
    "- Average\n",
    "- Min/feature\n",
    "- Max/feature\n",
    "\n",
    "#### a few pointers:\n",
    "\n",
    "- `w2v.wv.vocab` is a `set()` of the vocabulary (all existing words in your model)\n",
    "- `np.minimum(a,b) and np.maximum(a,b)` respectively return element-wise min/max "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  14.64355245   27.6113289     2.8425088    -7.93338127    6.84360613\n",
      "  -79.19433906   10.86327781  196.89161115  -67.31446975  -92.96547834\n",
      "   15.56442079 -112.20022673    5.8086668    86.83066121  -25.40946327\n",
      "  -49.9276499    33.3363579   -69.70861041   -7.62340931 -215.95424779\n",
      "   50.17666742   -2.38020069  145.31848439  -69.1918506   -39.70970039\n",
      "   68.09699225  -56.2441937    27.71014114  -98.97472918   79.38693306\n",
      "   68.50543934    1.10013077   46.54545162 -145.75732527    6.4726288\n",
      "   62.17236457   59.48089204  -91.40268719  -69.07438376 -159.08528891\n",
      "   -4.02529158 -129.18004145  -47.1709022    36.39325455   72.80883827\n",
      "  -61.38499063  -51.53503226   15.24842464   25.49245624   25.80657919\n",
      "   40.06329639  -66.5347559    63.89715937  -29.40907955  -44.37395762\n",
      "  -11.74681224   -7.10754368   20.37445744  -33.31172254   70.73421621\n",
      "   55.71731006  -48.41107585  -10.93231426   51.54490925  -86.33590543\n",
      "   95.01279946   27.11545611   69.53969419  -94.11321586   62.86848952\n",
      "    0.43030673  101.30865796   73.77609326   -3.68136275  122.30405301\n",
      "   35.47705598   23.71823483   33.8453234   -23.72429856   38.16687463\n",
      "  -42.20675605    7.31931338  -99.88609182  122.68993982  -28.24487971\n",
      "    7.86717409   38.95840096   58.42231114  111.25336451   28.47953462\n",
      "  149.61692966   85.97479387   32.73059883  -40.12095338  175.6718829\n",
      "    5.41450242  111.27464552 -137.22331149    0.56803713  -38.1777231 ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# We first need to vectorize text:\n",
    "# First we propose to a sum of them\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def randomvec():\n",
    "    default = np.random.randn(100)\n",
    "    default = default  / np.linalg.norm(default)\n",
    "    return default\n",
    "\n",
    "def vectorize(text,func=np.sum):\n",
    "    \"\"\"\n",
    "    This function should vectorize one review\n",
    "\n",
    "    input: str\n",
    "    output: np.array(float)\n",
    "    \"\"\"    \n",
    "\n",
    "    #for word in text:\n",
    "        # do something\n",
    "    vec = []\n",
    "    for word in text:\n",
    "        if not (word in w2v.wv):\n",
    "            vec.append(randomvec())\n",
    "        else:\n",
    "            vec.append(w2v.wv[word])\n",
    "    #if mean:\n",
    "    #    return np.mean(vec,axis=0)\n",
    "    return func(np.array(vec),axis=0)\n",
    "    \n",
    "lab = [l for t,l in data]\n",
    "train,test, y_train, y_test = train_test_split(text,lab,test_size=0.2,random_state=42)\n",
    "#classes = [pol for text,pol in train]\n",
    "X_train = [vectorize(text) for text in train]\n",
    "X_test = [vectorize(text) for text in test]\n",
    "#true = [pol for text,pol in test]\n",
    "\n",
    "\n",
    "#let's see what a review vector looks like.\n",
    "print(X_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Train a classifier \n",
    "as in the previous practical session, train a logistic regression to do sentiment classification with word vectors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression()\n",
      "score sum= 0.8256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aylinsoykok/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "train,test, y_train, y_test = train_test_split(text,lab,test_size=0.2,random_state=42)\n",
    "\n",
    "X_train = [vectorize(text,func=np.sum) for text in train]\n",
    "X_test = [vectorize(text,func=np.sum) for text in test]\n",
    "\n",
    "# Scikit Logistic Regression\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train,  y_train)  \n",
    "score = clf.score(X_test,y_test)\n",
    "print(clf)\n",
    "print(\"score sum=\",score)\n",
    "#print(\"classifier:\",clf.coef_) # retrieve the coefs from inside the object (cf doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression()\n",
      "score mean= 0.8156\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_train = [vectorize(text,func=np.mean) for text in train]\n",
    "X_test = [vectorize(text,func=np.mean) for text in test]\n",
    "clf1 = LogisticRegression()\n",
    "clf1.fit(X_train,  y_train)  \n",
    "score = clf1.score(X_test,y_test)\n",
    "print(clf1)\n",
    "print(\"score mean=\",score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression()\n",
      "score max= 0.707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aylinsoykok/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "X_train = [vectorize(text,func=np.max) for text in train]\n",
    "X_test = [vectorize(text,func=np.max) for text in test]\n",
    "clf2 = LogisticRegression()\n",
    "clf2.fit(X_train,  y_train)  \n",
    "score = clf2.score(X_test,y_test)\n",
    "print(clf2)\n",
    "print(\"score max=\",score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression()\n",
      "score min= 0.7046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aylinsoykok/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "X_train = [vectorize(text,func=np.min) for text in train]\n",
    "X_test = [vectorize(text,func=np.min) for text in test]\n",
    "clf3 = LogisticRegression()\n",
    "clf3.fit(X_train,  y_train)  \n",
    "score = clf3.score(X_test,y_test)\n",
    "print(clf3)\n",
    "print(\"score min=\",score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "performance should be worst than with bag of word (~80%). Sum/Mean aggregation does not work well on long reviews (especially with many frequent words). This adds a lot of noise.\n",
    "\n",
    "## **Todo**:  Try answering the following questions:\n",
    "\n",
    "- Which word2vec model works best: skip-gram or cbow\n",
    "- Do pretrained vectors work best than those learnt on the train dataset ?\n",
    "\n",
    "## **Todo**: evaluate the same pipeline on speaker ID task (Chirac/Mitterrand) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-27 21:22:27,600 : INFO : collecting all words and their counts\n",
      "2024-02-27 21:22:27,604 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2024-02-27 21:22:28,071 : INFO : PROGRESS: at sentence #10000, processed 2301366 words, keeping 153853 word types\n",
      "2024-02-27 21:22:28,564 : INFO : PROGRESS: at sentence #20000, processed 4553558 words, keeping 240043 word types\n",
      "2024-02-27 21:22:28,831 : INFO : collected 276678 word types from a corpus of 5713167 raw words and 25000 sentences\n",
      "2024-02-27 21:22:28,833 : INFO : Creating a fresh vocabulary\n",
      "2024-02-27 21:22:29,050 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 48208 unique words (17.42% of original 276678, drops 228470)', 'datetime': '2024-02-27T21:22:29.049973', 'gensim': '4.3.0', 'python': '3.8.17 (default, Jul  5 2023, 16:07:30) \\n[Clang 14.0.6 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "2024-02-27 21:22:29,051 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 5389596 word corpus (94.34% of original 5713167, drops 323571)', 'datetime': '2024-02-27T21:22:29.051032', 'gensim': '4.3.0', 'python': '3.8.17 (default, Jul  5 2023, 16:07:30) \\n[Clang 14.0.6 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "2024-02-27 21:22:29,278 : INFO : deleting the raw counts dictionary of 276678 items\n",
      "2024-02-27 21:22:29,283 : INFO : sample=0.001 downsamples 44 most-common words\n",
      "2024-02-27 21:22:29,284 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 4165010.0598832574 word corpus (77.3%% of prior 5389596)', 'datetime': '2024-02-27T21:22:29.284005', 'gensim': '4.3.0', 'python': '3.8.17 (default, Jul  5 2023, 16:07:30) \\n[Clang 14.0.6 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "2024-02-27 21:22:29,678 : INFO : estimated required memory for 48208 words and 100 dimensions: 62670400 bytes\n",
      "2024-02-27 21:22:29,678 : INFO : resetting layer weights\n",
      "2024-02-27 21:22:29,708 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-02-27T21:22:29.708057', 'gensim': '4.3.0', 'python': '3.8.17 (default, Jul  5 2023, 16:07:30) \\n[Clang 14.0.6 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'build_vocab'}\n",
      "2024-02-27 21:22:29,709 : INFO : Word2Vec lifecycle event {'msg': 'training model with 3 workers on 48208 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2024-02-27T21:22:29.709061', 'gensim': '4.3.0', 'python': '3.8.17 (default, Jul  5 2023, 16:07:30) \\n[Clang 14.0.6 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'train'}\n",
      "2024-02-27 21:22:30,723 : INFO : EPOCH 0 - PROGRESS: at 33.40% examples, 1387053 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 21:22:31,724 : INFO : EPOCH 0 - PROGRESS: at 67.07% examples, 1393819 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 21:22:32,700 : INFO : EPOCH 0: training on 5713167 raw words (4164785 effective words) took 3.0s, 1397384 effective words/s\n",
      "2024-02-27 21:22:33,705 : INFO : EPOCH 1 - PROGRESS: at 33.91% examples, 1409609 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 21:22:34,709 : INFO : EPOCH 1 - PROGRESS: at 67.98% examples, 1411174 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 21:22:35,654 : INFO : EPOCH 1: training on 5713167 raw words (4165056 effective words) took 3.0s, 1411015 effective words/s\n",
      "2024-02-27 21:22:36,661 : INFO : EPOCH 2 - PROGRESS: at 33.91% examples, 1408000 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 21:22:37,661 : INFO : EPOCH 2 - PROGRESS: at 66.90% examples, 1391354 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 21:22:38,647 : INFO : EPOCH 2: training on 5713167 raw words (4164478 effective words) took 3.0s, 1392981 effective words/s\n",
      "2024-02-27 21:22:39,652 : INFO : EPOCH 3 - PROGRESS: at 33.74% examples, 1401973 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 21:22:40,658 : INFO : EPOCH 3 - PROGRESS: at 67.83% examples, 1406071 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 21:22:41,603 : INFO : EPOCH 3: training on 5713167 raw words (4166000 effective words) took 3.0s, 1410634 effective words/s\n",
      "2024-02-27 21:22:42,608 : INFO : EPOCH 4 - PROGRESS: at 32.53% examples, 1350743 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 21:22:43,612 : INFO : EPOCH 4 - PROGRESS: at 66.55% examples, 1381380 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 21:22:44,582 : INFO : EPOCH 4: training on 5713167 raw words (4163903 effective words) took 3.0s, 1398515 effective words/s\n",
      "2024-02-27 21:22:44,583 : INFO : Word2Vec lifecycle event {'msg': 'training on 28565835 raw words (20824222 effective words) took 14.9s, 1400045 effective words/s', 'datetime': '2024-02-27T21:22:44.583682', 'gensim': '4.3.0', 'python': '3.8.17 (default, Jul  5 2023, 16:07:30) \\n[Clang 14.0.6 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'train'}\n",
      "2024-02-27 21:22:44,584 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=48208, vector_size=100, alpha=0.025>', 'datetime': '2024-02-27T21:22:44.584197', 'gensim': '4.3.0', 'python': '3.8.17 (default, Jul  5 2023, 16:07:30) \\n[Clang 14.0.6 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "#- Which word2vec model works best: skip-gram or cbow\n",
    "skipgram = w2v\n",
    "cbow =gensim.models.word2vec.Word2Vec(sentences=text,\n",
    "                                vector_size=100, window=5,               \n",
    "                                min_count=5,                      \n",
    "                                sample=0.001, workers=3,\n",
    "                                sg=0, hs=0, negative=5,   # sg=0    \n",
    "                                cbow_mean=1, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S [('film', 0.9289810061454773), ('\"movie\"', 0.8283028602600098), ('flick', 0.7667436599731445), ('movie,', 0.7451980710029602), ('\"film\"', 0.7428070902824402)]\n",
      "C [('film', 0.9295904040336609), ('movie,', 0.8304710388183594), ('film,', 0.7681915163993835), ('flick', 0.747571587562561), ('documentary', 0.7352034449577332)]\n",
      "S [('amazing', 0.7817275524139404), ('excellent', 0.7275443077087402), ('awesome,', 0.6990002989768982), ('exceptional', 0.6928319931030273), ('cool', 0.6804935336112976)]\n",
      "C [('amazing', 0.8534489870071411), ('excellent', 0.8053512573242188), ('exceptional', 0.7795264720916748), ('incredible', 0.779024600982666), ('outstanding', 0.7754307985305786)]\n",
      "S [('actor,', 0.8143014907836914), ('actor.', 0.7617161273956299), ('Reeves', 0.7540039420127869), ('Hopper', 0.7491146326065063), ('actress', 0.734898567199707)]\n",
      "C [('actress', 0.8344793319702148), ('actor,', 0.8023119568824768), ('role', 0.7676254510879517), ('role,', 0.7433274984359741), ('performance', 0.7101996541023254)]\n"
     ]
    }
   ],
   "source": [
    "print(\"S\",skipgram.wv.most_similar(\"movie\",topn=5)) # 5 most similar words\n",
    "print(\"C\",cbow.wv.most_similar(\"movie\",topn=5)) # 5 most similar words\n",
    "print(\"S\",skipgram.wv.most_similar(\"awesome\",topn=5))\n",
    "print(\"C\",cbow.wv.most_similar(\"awesome\",topn=5))\n",
    "print(\"S\",skipgram.wv.most_similar(\"actor\",topn=5))\n",
    "print(\"C\",cbow.wv.most_similar(\"actor\",topn=5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cbow marche mieux avec un peu de pretraitement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-27 21:48:58,082 : INFO : Evaluating word analogies for top 300000 words in the model on /Users/aylinsoykok/opt/anaconda3/lib/python3.8/site-packages/gensim/test/test_data/questions-words.txt\n",
      "2024-02-27 21:48:58,166 : INFO : capital-common-countries: 2.2% (2/90)\n",
      "2024-02-27 21:48:58,236 : INFO : capital-world: 1.4% (1/71)\n",
      "2024-02-27 21:48:58,265 : INFO : currency: 0.0% (0/28)\n",
      "2024-02-27 21:48:58,567 : INFO : city-in-state: 0.0% (0/329)\n",
      "2024-02-27 21:48:58,880 : INFO : family: 36.8% (126/342)\n",
      "2024-02-27 21:48:59,695 : INFO : gram1-adjective-to-adverb: 2.7% (25/930)\n",
      "2024-02-27 21:49:00,191 : INFO : gram2-opposite: 3.1% (17/552)\n",
      "2024-02-27 21:49:01,218 : INFO : gram3-comparative: 22.1% (278/1260)\n",
      "2024-02-27 21:49:01,771 : INFO : gram4-superlative: 5.6% (39/702)\n",
      "2024-02-27 21:49:02,415 : INFO : gram5-present-participle: 17.1% (129/756)\n",
      "2024-02-27 21:49:03,140 : INFO : gram6-nationality-adjective: 3.4% (27/792)\n",
      "2024-02-27 21:49:04,233 : INFO : gram7-past-tense: 16.5% (208/1260)\n",
      "2024-02-27 21:49:04,906 : INFO : gram8-plural: 6.7% (54/812)\n",
      "2024-02-27 21:49:05,607 : INFO : gram9-plural-verbs: 27.1% (205/756)\n",
      "2024-02-27 21:49:05,608 : INFO : Quadruplets with out-of-vocabulary words: 55.6%\n",
      "2024-02-27 21:49:05,609 : INFO : NB: analogies containing OOV words were skipped from evaluation! To change this behavior, use \"dummy4unknown=True\"\n",
      "2024-02-27 21:49:05,609 : INFO : Total accuracy: 12.8% (1111/8680)\n",
      "2024-02-27 21:49:05,632 : INFO : Evaluating word analogies for top 300000 words in the model on /Users/aylinsoykok/opt/anaconda3/lib/python3.8/site-packages/gensim/test/test_data/questions-words.txt\n",
      "2024-02-27 21:49:05,724 : INFO : capital-common-countries: 4.4% (4/90)\n",
      "2024-02-27 21:49:05,806 : INFO : capital-world: 1.4% (1/71)\n",
      "2024-02-27 21:49:05,837 : INFO : currency: 0.0% (0/28)\n",
      "2024-02-27 21:49:06,185 : INFO : city-in-state: 0.0% (0/329)\n",
      "2024-02-27 21:49:06,530 : INFO : family: 39.5% (135/342)\n",
      "2024-02-27 21:49:07,470 : INFO : gram1-adjective-to-adverb: 1.4% (13/930)\n",
      "2024-02-27 21:49:08,025 : INFO : gram2-opposite: 4.0% (22/552)\n",
      "2024-02-27 21:49:09,314 : INFO : gram3-comparative: 21.5% (271/1260)\n",
      "2024-02-27 21:49:10,035 : INFO : gram4-superlative: 7.8% (55/702)\n",
      "2024-02-27 21:49:10,786 : INFO : gram5-present-participle: 15.2% (115/756)\n",
      "2024-02-27 21:49:11,592 : INFO : gram6-nationality-adjective: 1.0% (8/792)\n",
      "2024-02-27 21:49:12,822 : INFO : gram7-past-tense: 15.6% (196/1260)\n",
      "2024-02-27 21:49:13,608 : INFO : gram8-plural: 3.9% (32/812)\n",
      "2024-02-27 21:49:14,390 : INFO : gram9-plural-verbs: 18.1% (137/756)\n",
      "2024-02-27 21:49:14,391 : INFO : Quadruplets with out-of-vocabulary words: 55.6%\n",
      "2024-02-27 21:49:14,392 : INFO : NB: analogies containing OOV words were skipped from evaluation! To change this behavior, use \"dummy4unknown=True\"\n",
      "2024-02-27 21:49:14,392 : INFO : Total accuracy: 11.4% (989/8680)\n",
      "2024-02-27 21:49:14,419 : INFO : Skipping line #15 with OOV words: cucumber\tpotato\t5.92\n",
      "2024-02-27 21:49:14,420 : INFO : Skipping line #25 with OOV words: stock\tjaguar\t0.92\n",
      "2024-02-27 21:49:14,421 : INFO : Skipping line #27 with OOV words: fertility\tegg\t6.69\n",
      "2024-02-27 21:49:14,422 : INFO : Skipping line #34 with OOV words: professor\tcucumber\t0.31\n",
      "2024-02-27 21:49:14,422 : INFO : Skipping line #35 with OOV words: king\tcabbage\t0.23\n",
      "2024-02-27 21:49:14,423 : INFO : Skipping line #37 with OOV words: king\trook\t5.92\n",
      "2024-02-27 21:49:14,424 : INFO : Skipping line #39 with OOV words: Jerusalem\tIsrael\t8.46\n",
      "2024-02-27 21:49:14,424 : INFO : Skipping line #40 with OOV words: Jerusalem\tPalestinian\t7.65\n",
      "2024-02-27 21:49:14,425 : INFO : Skipping line #42 with OOV words: fuck\tsex\t9.44\n",
      "2024-02-27 21:49:14,425 : INFO : Skipping line #43 with OOV words: Maradona\tfootball\t8.62\n",
      "2024-02-27 21:49:14,426 : INFO : Skipping line #48 with OOV words: Arafat\tpeace\t6.73\n",
      "2024-02-27 21:49:14,427 : INFO : Skipping line #49 with OOV words: Arafat\tterror\t7.65\n",
      "2024-02-27 21:49:14,427 : INFO : Skipping line #50 with OOV words: Arafat\tJackson\t2.50\n",
      "2024-02-27 21:49:14,428 : INFO : Skipping line #56 with OOV words: physics\tproton\t8.12\n",
      "2024-02-27 21:49:14,428 : INFO : Skipping line #60 with OOV words: vodka\tgin\t8.46\n",
      "2024-02-27 21:49:14,429 : INFO : Skipping line #61 with OOV words: vodka\tbrandy\t8.13\n",
      "2024-02-27 21:49:14,430 : INFO : Skipping line #73 with OOV words: asylum\tmadhouse\t8.87\n",
      "2024-02-27 21:49:14,431 : INFO : Skipping line #75 with OOV words: midday\tnoon\t9.29\n",
      "2024-02-27 21:49:14,431 : INFO : Skipping line #76 with OOV words: furnace\tstove\t8.79\n",
      "2024-02-27 21:49:14,432 : INFO : Skipping line #85 with OOV words: monk\toracle\t5.00\n",
      "2024-02-27 21:49:14,433 : INFO : Skipping line #87 with OOV words: food\trooster\t4.42\n",
      "2024-02-27 21:49:14,434 : INFO : Skipping line #96 with OOV words: noon\tstring\t0.54\n",
      "2024-02-27 21:49:14,434 : INFO : Skipping line #97 with OOV words: rooster\tvoyage\t0.62\n",
      "2024-02-27 21:49:14,436 : INFO : Skipping line #107 with OOV words: money\tlaundering\t5.65\n",
      "2024-02-27 21:49:14,437 : INFO : Skipping line #109 with OOV words: tiger\tjaguar\t8.00\n",
      "2024-02-27 21:49:14,437 : INFO : Skipping line #111 with OOV words: tiger\tcarnivore\t7.08\n",
      "2024-02-27 21:49:14,438 : INFO : Skipping line #112 with OOV words: tiger\tmammal\t6.85\n",
      "2024-02-27 21:49:14,438 : INFO : Skipping line #114 with OOV words: tiger\torganism\t4.77\n",
      "2024-02-27 21:49:14,439 : INFO : Skipping line #115 with OOV words: tiger\tfauna\t5.62\n",
      "2024-02-27 21:49:14,439 : INFO : Skipping line #117 with OOV words: psychology\tpsychiatry\t8.08\n",
      "2024-02-27 21:49:14,441 : INFO : Skipping line #128 with OOV words: psychology\tcognition\t7.48\n",
      "2024-02-27 21:49:14,442 : INFO : Skipping line #135 with OOV words: planet\tastronomer\t7.94\n",
      "2024-02-27 21:49:14,442 : INFO : Skipping line #136 with OOV words: precedent\texample\t5.85\n",
      "2024-02-27 21:49:14,443 : INFO : Skipping line #137 with OOV words: precedent\tinformation\t3.85\n",
      "2024-02-27 21:49:14,444 : INFO : Skipping line #138 with OOV words: precedent\tcognition\t2.81\n",
      "2024-02-27 21:49:14,444 : INFO : Skipping line #139 with OOV words: precedent\tlaw\t6.65\n",
      "2024-02-27 21:49:14,445 : INFO : Skipping line #140 with OOV words: precedent\tcollection\t2.50\n",
      "2024-02-27 21:49:14,445 : INFO : Skipping line #141 with OOV words: precedent\tgroup\t1.77\n",
      "2024-02-27 21:49:14,446 : INFO : Skipping line #142 with OOV words: precedent\tantecedent\t6.04\n",
      "2024-02-27 21:49:14,447 : INFO : Skipping line #144 with OOV words: cup\ttableware\t6.85\n",
      "2024-02-27 21:49:14,448 : INFO : Skipping line #153 with OOV words: jaguar\tcat\t7.42\n",
      "2024-02-27 21:49:14,448 : INFO : Skipping line #154 with OOV words: jaguar\tcar\t7.27\n",
      "2024-02-27 21:49:14,450 : INFO : Skipping line #160 with OOV words: FBI\tfingerprint\t6.94\n",
      "2024-02-27 21:49:14,451 : INFO : Skipping line #169 with OOV words: water\tseepage\t6.56\n",
      "2024-02-27 21:49:14,452 : INFO : Skipping line #170 with OOV words: sign\trecess\t2.38\n",
      "2024-02-27 21:49:14,452 : INFO : Skipping line #172 with OOV words: mile\tkilometer\t8.66\n",
      "2024-02-27 21:49:14,454 : INFO : Skipping line #184 with OOV words: decoration\tvalor\t5.63\n",
      "2024-02-27 21:49:14,456 : INFO : Skipping line #205 with OOV words: doctor\tliability\t5.19\n",
      "2024-02-27 21:49:14,457 : INFO : Skipping line #206 with OOV words: liability\tinsurance\t7.03\n",
      "2024-02-27 21:49:14,457 : INFO : Skipping line #208 with OOV words: reason\thypertension\t2.31\n",
      "2024-02-27 21:49:14,458 : INFO : Skipping line #212 with OOV words: hospital\tinfrastructure\t4.63\n",
      "2024-02-27 21:49:14,459 : INFO : Skipping line #221 with OOV words: OPEC\tcountry\t5.63\n",
      "2024-02-27 21:49:14,460 : INFO : Skipping line #224 with OOV words: territory\tkilometer\t5.28\n",
      "2024-02-27 21:49:14,461 : INFO : Skipping line #235 with OOV words: registration\tarrangement\t6.00\n",
      "2024-02-27 21:49:14,462 : INFO : Skipping line #236 with OOV words: arrangement\taccommodation\t5.41\n",
      "2024-02-27 21:49:14,463 : INFO : Skipping line #244 with OOV words: impartiality\tinterest\t5.16\n",
      "2024-02-27 21:49:14,464 : INFO : Skipping line #253 with OOV words: production\thike\t1.75\n",
      "2024-02-27 21:49:14,464 : INFO : Skipping line #257 with OOV words: dividend\tpayment\t7.63\n",
      "2024-02-27 21:49:14,465 : INFO : Skipping line #258 with OOV words: dividend\tcalculation\t6.48\n",
      "2024-02-27 21:49:14,465 : INFO : Skipping line #259 with OOV words: calculation\tcomputation\t8.44\n",
      "2024-02-27 21:49:14,466 : INFO : Skipping line #261 with OOV words: OPEC\toil\t8.59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-27 21:49:14,468 : INFO : Skipping line #278 with OOV words: investor\tearning\t7.13\n",
      "2024-02-27 21:49:14,468 : INFO : Skipping line #283 with OOV words: marathon\tsprint\t7.47\n",
      "2024-02-27 21:49:14,469 : INFO : Skipping line #287 with OOV words: seafood\tsea\t7.47\n",
      "2024-02-27 21:49:14,470 : INFO : Skipping line #288 with OOV words: seafood\tfood\t8.34\n",
      "2024-02-27 21:49:14,470 : INFO : Skipping line #289 with OOV words: seafood\tlobster\t8.70\n",
      "2024-02-27 21:49:14,472 : INFO : Skipping line #306 with OOV words: environment\tecology\t8.81\n",
      "2024-02-27 21:49:14,473 : INFO : Skipping line #310 with OOV words: murder\tmanslaughter\t8.53\n",
      "2024-02-27 21:49:14,475 : INFO : Skipping line #330 with OOV words: ministry\tculture\t4.69\n",
      "2024-02-27 21:49:14,476 : INFO : Skipping line #342 with OOV words: concert\tvirtuoso\t6.81\n",
      "2024-02-27 21:49:14,477 : INFO : Skipping line #351 with OOV words: weather\tforecast\t8.34\n",
      "2024-02-27 21:49:14,482 : INFO : Pearson correlation coefficient against /Users/aylinsoykok/opt/anaconda3/lib/python3.8/site-packages/gensim/test/test_data/wordsim353.tsv: 0.2339\n",
      "2024-02-27 21:49:14,482 : INFO : Spearman rank-order correlation coefficient against /Users/aylinsoykok/opt/anaconda3/lib/python3.8/site-packages/gensim/test/test_data/wordsim353.tsv: 0.2324\n",
      "2024-02-27 21:49:14,483 : INFO : Pairs with unknown words ratio: 20.1%\n",
      "2024-02-27 21:49:14,514 : INFO : Skipping line #15 with OOV words: cucumber\tpotato\t5.92\n",
      "2024-02-27 21:49:14,516 : INFO : Skipping line #25 with OOV words: stock\tjaguar\t0.92\n",
      "2024-02-27 21:49:14,516 : INFO : Skipping line #27 with OOV words: fertility\tegg\t6.69\n",
      "2024-02-27 21:49:14,518 : INFO : Skipping line #34 with OOV words: professor\tcucumber\t0.31\n",
      "2024-02-27 21:49:14,519 : INFO : Skipping line #35 with OOV words: king\tcabbage\t0.23\n",
      "2024-02-27 21:49:14,519 : INFO : Skipping line #37 with OOV words: king\trook\t5.92\n",
      "2024-02-27 21:49:14,520 : INFO : Skipping line #39 with OOV words: Jerusalem\tIsrael\t8.46\n",
      "2024-02-27 21:49:14,520 : INFO : Skipping line #40 with OOV words: Jerusalem\tPalestinian\t7.65\n",
      "2024-02-27 21:49:14,521 : INFO : Skipping line #42 with OOV words: fuck\tsex\t9.44\n",
      "2024-02-27 21:49:14,521 : INFO : Skipping line #43 with OOV words: Maradona\tfootball\t8.62\n",
      "2024-02-27 21:49:14,523 : INFO : Skipping line #48 with OOV words: Arafat\tpeace\t6.73\n",
      "2024-02-27 21:49:14,523 : INFO : Skipping line #49 with OOV words: Arafat\tterror\t7.65\n",
      "2024-02-27 21:49:14,524 : INFO : Skipping line #50 with OOV words: Arafat\tJackson\t2.50\n",
      "2024-02-27 21:49:14,525 : INFO : Skipping line #56 with OOV words: physics\tproton\t8.12\n",
      "2024-02-27 21:49:14,526 : INFO : Skipping line #60 with OOV words: vodka\tgin\t8.46\n",
      "2024-02-27 21:49:14,526 : INFO : Skipping line #61 with OOV words: vodka\tbrandy\t8.13\n",
      "2024-02-27 21:49:14,528 : INFO : Skipping line #73 with OOV words: asylum\tmadhouse\t8.87\n",
      "2024-02-27 21:49:14,529 : INFO : Skipping line #75 with OOV words: midday\tnoon\t9.29\n",
      "2024-02-27 21:49:14,529 : INFO : Skipping line #76 with OOV words: furnace\tstove\t8.79\n",
      "2024-02-27 21:49:14,531 : INFO : Skipping line #85 with OOV words: monk\toracle\t5.00\n",
      "2024-02-27 21:49:14,531 : INFO : Skipping line #87 with OOV words: food\trooster\t4.42\n",
      "2024-02-27 21:49:14,533 : INFO : Skipping line #96 with OOV words: noon\tstring\t0.54\n",
      "2024-02-27 21:49:14,533 : INFO : Skipping line #97 with OOV words: rooster\tvoyage\t0.62\n",
      "2024-02-27 21:49:14,535 : INFO : Skipping line #107 with OOV words: money\tlaundering\t5.65\n",
      "2024-02-27 21:49:14,536 : INFO : Skipping line #109 with OOV words: tiger\tjaguar\t8.00\n",
      "2024-02-27 21:49:14,536 : INFO : Skipping line #111 with OOV words: tiger\tcarnivore\t7.08\n",
      "2024-02-27 21:49:14,537 : INFO : Skipping line #112 with OOV words: tiger\tmammal\t6.85\n",
      "2024-02-27 21:49:14,537 : INFO : Skipping line #114 with OOV words: tiger\torganism\t4.77\n",
      "2024-02-27 21:49:14,537 : INFO : Skipping line #115 with OOV words: tiger\tfauna\t5.62\n",
      "2024-02-27 21:49:14,539 : INFO : Skipping line #117 with OOV words: psychology\tpsychiatry\t8.08\n",
      "2024-02-27 21:49:14,543 : INFO : Skipping line #128 with OOV words: psychology\tcognition\t7.48\n",
      "2024-02-27 21:49:14,544 : INFO : Skipping line #135 with OOV words: planet\tastronomer\t7.94\n",
      "2024-02-27 21:49:14,545 : INFO : Skipping line #136 with OOV words: precedent\texample\t5.85\n",
      "2024-02-27 21:49:14,545 : INFO : Skipping line #137 with OOV words: precedent\tinformation\t3.85\n",
      "2024-02-27 21:49:14,546 : INFO : Skipping line #138 with OOV words: precedent\tcognition\t2.81\n",
      "2024-02-27 21:49:14,546 : INFO : Skipping line #139 with OOV words: precedent\tlaw\t6.65\n",
      "2024-02-27 21:49:14,547 : INFO : Skipping line #140 with OOV words: precedent\tcollection\t2.50\n",
      "2024-02-27 21:49:14,548 : INFO : Skipping line #141 with OOV words: precedent\tgroup\t1.77\n",
      "2024-02-27 21:49:14,549 : INFO : Skipping line #142 with OOV words: precedent\tantecedent\t6.04\n",
      "2024-02-27 21:49:14,549 : INFO : Skipping line #144 with OOV words: cup\ttableware\t6.85\n",
      "2024-02-27 21:49:14,552 : INFO : Skipping line #153 with OOV words: jaguar\tcat\t7.42\n",
      "2024-02-27 21:49:14,553 : INFO : Skipping line #154 with OOV words: jaguar\tcar\t7.27\n",
      "2024-02-27 21:49:14,554 : INFO : Skipping line #160 with OOV words: FBI\tfingerprint\t6.94\n",
      "2024-02-27 21:49:14,556 : INFO : Skipping line #169 with OOV words: water\tseepage\t6.56\n",
      "2024-02-27 21:49:14,557 : INFO : Skipping line #170 with OOV words: sign\trecess\t2.38\n",
      "2024-02-27 21:49:14,558 : INFO : Skipping line #172 with OOV words: mile\tkilometer\t8.66\n",
      "2024-02-27 21:49:14,560 : INFO : Skipping line #184 with OOV words: decoration\tvalor\t5.63\n",
      "2024-02-27 21:49:14,563 : INFO : Skipping line #205 with OOV words: doctor\tliability\t5.19\n",
      "2024-02-27 21:49:14,563 : INFO : Skipping line #206 with OOV words: liability\tinsurance\t7.03\n",
      "2024-02-27 21:49:14,564 : INFO : Skipping line #208 with OOV words: reason\thypertension\t2.31\n",
      "2024-02-27 21:49:14,565 : INFO : Skipping line #212 with OOV words: hospital\tinfrastructure\t4.63\n",
      "2024-02-27 21:49:14,567 : INFO : Skipping line #221 with OOV words: OPEC\tcountry\t5.63\n",
      "2024-02-27 21:49:14,568 : INFO : Skipping line #224 with OOV words: territory\tkilometer\t5.28\n",
      "2024-02-27 21:49:14,570 : INFO : Skipping line #235 with OOV words: registration\tarrangement\t6.00\n",
      "2024-02-27 21:49:14,571 : INFO : Skipping line #236 with OOV words: arrangement\taccommodation\t5.41\n",
      "2024-02-27 21:49:14,572 : INFO : Skipping line #244 with OOV words: impartiality\tinterest\t5.16\n",
      "2024-02-27 21:49:14,573 : INFO : Skipping line #253 with OOV words: production\thike\t1.75\n",
      "2024-02-27 21:49:14,574 : INFO : Skipping line #257 with OOV words: dividend\tpayment\t7.63\n",
      "2024-02-27 21:49:14,575 : INFO : Skipping line #258 with OOV words: dividend\tcalculation\t6.48\n",
      "2024-02-27 21:49:14,575 : INFO : Skipping line #259 with OOV words: calculation\tcomputation\t8.44\n",
      "2024-02-27 21:49:14,576 : INFO : Skipping line #261 with OOV words: OPEC\toil\t8.59\n",
      "2024-02-27 21:49:14,579 : INFO : Skipping line #278 with OOV words: investor\tearning\t7.13\n",
      "2024-02-27 21:49:14,579 : INFO : Skipping line #283 with OOV words: marathon\tsprint\t7.47\n",
      "2024-02-27 21:49:14,580 : INFO : Skipping line #287 with OOV words: seafood\tsea\t7.47\n",
      "2024-02-27 21:49:14,581 : INFO : Skipping line #288 with OOV words: seafood\tfood\t8.34\n",
      "2024-02-27 21:49:14,581 : INFO : Skipping line #289 with OOV words: seafood\tlobster\t8.70\n",
      "2024-02-27 21:49:14,583 : INFO : Skipping line #306 with OOV words: environment\tecology\t8.81\n",
      "2024-02-27 21:49:14,583 : INFO : Skipping line #310 with OOV words: murder\tmanslaughter\t8.53\n",
      "2024-02-27 21:49:14,588 : INFO : Skipping line #330 with OOV words: ministry\tculture\t4.69\n",
      "2024-02-27 21:49:14,589 : INFO : Skipping line #342 with OOV words: concert\tvirtuoso\t6.81\n",
      "2024-02-27 21:49:14,591 : INFO : Skipping line #351 with OOV words: weather\tforecast\t8.34\n",
      "2024-02-27 21:49:14,595 : INFO : Pearson correlation coefficient against /Users/aylinsoykok/opt/anaconda3/lib/python3.8/site-packages/gensim/test/test_data/wordsim353.tsv: 0.1773\n",
      "2024-02-27 21:49:14,596 : INFO : Spearman rank-order correlation coefficient against /Users/aylinsoykok/opt/anaconda3/lib/python3.8/site-packages/gensim/test/test_data/wordsim353.tsv: 0.1544\n",
      "2024-02-27 21:49:14,596 : INFO : Pairs with unknown words ratio: 20.1%\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "\n",
    "from gensim.test.utils import datapath\n",
    "analogy_skipgram_score = skipgram.wv.evaluate_word_analogies(datapath('questions-words.txt'))[0]\n",
    "analogy_cbow_score = cbow.wv.evaluate_word_analogies(datapath('questions-words.txt'))[0]\n",
    "\n",
    "similarity_skipgram_score = skipgram.wv.evaluate_word_pairs(datapath('wordsim353.tsv'))[0]\n",
    "similarity_cbow_score = cbow.wv.evaluate_word_pairs(datapath('wordsim353.tsv'))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analogy score (skip-gram): 0.12799539170506913\n",
      "Analogy score (cbow): 0.11394009216589862\n",
      "Similarity score (skip-gram): PearsonRResult(statistic=0.23389082080183135, pvalue=7.327643069840452e-05)\n",
      "Similarity score (cbow): PearsonRResult(statistic=0.17728023181783326, pvalue=0.002812533933874721)\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "\n",
    "from gensim.test.utils import datapath\n",
    "print(\"Analogy score (skip-gram):\",analogy_skipgram_score)\n",
    "print(\"Analogy score (cbow):\",analogy_cbow_score)\n",
    "\n",
    "print(\"Similarity score (skip-gram):\", similarity_skipgram_score)\n",
    "print(\"Similarity score (cbow):\", similarity_cbow_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation avec differents méthodes d'aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_new(text,func,model):\n",
    "    vec = []\n",
    "    for word in text:\n",
    "        if not (word in model.wv):\n",
    "            vec.append(randomvec())\n",
    "        else:\n",
    "            vec.append(model.wv[word])         \n",
    "    return func(np.array(vec),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(func, model):\n",
    "    train,test, y_train, y_test = train_test_split(text,lab,test_size=0.2,random_state=42)\n",
    "    X_train = [vectorize_new(text,func,model) for text in train]\n",
    "    X_test = [vectorize_new(text,func,model) for text in test]\n",
    "    clf = LogisticRegression()\n",
    "    clf.fit(X_train,  y_train)  \n",
    "    score = clf.score(X_test,y_test)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing(model_sg,model_cbow,aggreg_functions):\n",
    "    print(\"Results:\")\n",
    "    print(\"=\"*50)\n",
    "    print(\"{:<10} {:<15} {:<15}\".format(\"Method\", \"skip-gram\", \"cbow\"))\n",
    "    print(\"=\"*50)\n",
    "    for func in aggreg_functions:\n",
    "        score_sg = evaluation(func, model_sg)\n",
    "        score_cbow = evaluation(func, model_cbow)\n",
    "        print(\"{:<10} {:<15.4f} {:<15.4f}\".format(func.__name__, score_sg, score_cbow))\n",
    "    print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results:\n",
      "==================================================\n",
      "Method     skip-gram       cbow           \n",
      "==================================================\n",
      "sum        0.8208          0.7762         \n",
      "mean       0.8174          0.7786         \n",
      "amax       0.7078          0.6590         \n",
      "amin       0.7054          0.6468         \n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "aggregation_functions = [np.sum,np.mean,np.max,np.min]\n",
    "testing(skipgram,cbow,aggregation_functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**(Bonus)** To have a better accuracy, we could try two things:\n",
    "- Better aggregation methods (weight by tf-idf ?)\n",
    "- Another word vectorizing method such as [fasttext](https://radimrehurek.com/gensim/models/fasttext.html)\n",
    "- A document vectorizing method such as [Doc2Vec](https://radimrehurek.com/gensim/models/doc2vec.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-27 22:16:46,815 : INFO : collecting all words and their counts\n",
      "2024-02-27 22:16:46,816 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2024-02-27 22:16:47,271 : INFO : PROGRESS: at sentence #10000, processed 2301366 words, keeping 153853 word types\n",
      "2024-02-27 22:16:47,704 : INFO : PROGRESS: at sentence #20000, processed 4553558 words, keeping 240043 word types\n",
      "2024-02-27 22:16:47,950 : INFO : collected 276678 word types from a corpus of 5713167 raw words and 25000 sentences\n",
      "2024-02-27 22:16:47,951 : INFO : Creating a fresh vocabulary\n",
      "2024-02-27 22:16:48,130 : INFO : FastText lifecycle event {'msg': 'effective_min_count=5 retains 48208 unique words (17.42% of original 276678, drops 228470)', 'datetime': '2024-02-27T22:16:48.130563', 'gensim': '4.3.0', 'python': '3.8.17 (default, Jul  5 2023, 16:07:30) \\n[Clang 14.0.6 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "2024-02-27 22:16:48,131 : INFO : FastText lifecycle event {'msg': 'effective_min_count=5 leaves 5389596 word corpus (94.34% of original 5713167, drops 323571)', 'datetime': '2024-02-27T22:16:48.131447', 'gensim': '4.3.0', 'python': '3.8.17 (default, Jul  5 2023, 16:07:30) \\n[Clang 14.0.6 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "2024-02-27 22:16:48,309 : INFO : deleting the raw counts dictionary of 276678 items\n",
      "2024-02-27 22:16:48,314 : INFO : sample=0.001 downsamples 44 most-common words\n",
      "2024-02-27 22:16:48,315 : INFO : FastText lifecycle event {'msg': 'downsampling leaves estimated 4165010.0598832574 word corpus (77.3%% of prior 5389596)', 'datetime': '2024-02-27T22:16:48.315124', 'gensim': '4.3.0', 'python': '3.8.17 (default, Jul  5 2023, 16:07:30) \\n[Clang 14.0.6 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "2024-02-27 22:16:48,944 : INFO : estimated required memory for 48208 words, 2000000 buckets and 100 dimensions: 871842484 bytes\n",
      "2024-02-27 22:16:48,944 : INFO : resetting layer weights\n",
      "2024-02-27 22:16:51,669 : INFO : FastText lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-02-27T22:16:51.669091', 'gensim': '4.3.0', 'python': '3.8.17 (default, Jul  5 2023, 16:07:30) \\n[Clang 14.0.6 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'build_vocab'}\n",
      "2024-02-27 22:16:51,670 : INFO : FastText lifecycle event {'msg': 'training model with 3 workers on 48208 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2024-02-27T22:16:51.670049', 'gensim': '4.3.0', 'python': '3.8.17 (default, Jul  5 2023, 16:07:30) \\n[Clang 14.0.6 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'train'}\n",
      "2024-02-27 22:16:52,746 : INFO : EPOCH 0 - PROGRESS: at 5.73% examples, 226434 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 22:16:53,791 : INFO : EPOCH 0 - PROGRESS: at 11.92% examples, 236229 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 22:16:54,807 : INFO : EPOCH 0 - PROGRESS: at 18.17% examples, 241279 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 22:16:55,826 : INFO : EPOCH 0 - PROGRESS: at 24.28% examples, 243905 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 22:16:56,849 : INFO : EPOCH 0 - PROGRESS: at 30.53% examples, 245376 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 22:16:57,861 : INFO : EPOCH 0 - PROGRESS: at 36.38% examples, 245612 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 22:16:58,880 : INFO : EPOCH 0 - PROGRESS: at 42.75% examples, 246582 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 22:16:59,903 : INFO : EPOCH 0 - PROGRESS: at 48.72% examples, 246209 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 22:17:00,926 : INFO : EPOCH 0 - PROGRESS: at 55.05% examples, 246795 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 22:17:01,929 : INFO : EPOCH 0 - PROGRESS: at 60.96% examples, 247217 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 22:17:02,941 : INFO : EPOCH 0 - PROGRESS: at 67.07% examples, 248037 words/s, in_qsize 6, out_qsize 0\n",
      "2024-02-27 22:17:03,949 : INFO : EPOCH 0 - PROGRESS: at 73.45% examples, 248814 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 22:17:04,959 : INFO : EPOCH 0 - PROGRESS: at 79.80% examples, 248931 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 22:17:05,960 : INFO : EPOCH 0 - PROGRESS: at 85.69% examples, 249094 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 22:17:06,975 : INFO : EPOCH 0 - PROGRESS: at 91.86% examples, 249497 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 22:17:08,034 : INFO : EPOCH 0 - PROGRESS: at 97.98% examples, 249168 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 22:17:08,368 : INFO : EPOCH 0: training on 5713167 raw words (4164094 effective words) took 16.7s, 249417 effective words/s\n",
      "2024-02-27 22:17:09,411 : INFO : EPOCH 1 - PROGRESS: at 5.73% examples, 233662 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 22:17:10,452 : INFO : EPOCH 1 - PROGRESS: at 11.92% examples, 240460 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 22:17:11,471 : INFO : EPOCH 1 - PROGRESS: at 18.17% examples, 243979 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 22:17:12,504 : INFO : EPOCH 1 - PROGRESS: at 24.30% examples, 245156 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 22:17:13,527 : INFO : EPOCH 1 - PROGRESS: at 30.53% examples, 246330 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 22:17:14,539 : INFO : EPOCH 1 - PROGRESS: at 36.52% examples, 247569 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 22:17:15,583 : INFO : EPOCH 1 - PROGRESS: at 42.91% examples, 247451 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 22:17:16,585 : INFO : EPOCH 1 - PROGRESS: at 49.08% examples, 248446 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 22:17:17,592 : INFO : EPOCH 1 - PROGRESS: at 55.40% examples, 249267 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 22:17:18,613 : INFO : EPOCH 1 - PROGRESS: at 61.50% examples, 249715 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 22:17:19,662 : INFO : EPOCH 1 - PROGRESS: at 67.61% examples, 249511 words/s, in_qsize 6, out_qsize 0\n",
      "2024-02-27 22:17:20,684 : INFO : EPOCH 1 - PROGRESS: at 73.99% examples, 249935 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 22:17:21,701 : INFO : EPOCH 1 - PROGRESS: at 80.49% examples, 250357 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 22:17:22,728 : INFO : EPOCH 1 - PROGRESS: at 86.65% examples, 250466 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 22:17:23,773 : INFO : EPOCH 1 - PROGRESS: at 92.67% examples, 250268 words/s, in_qsize 6, out_qsize 0\n",
      "2024-02-27 22:17:24,773 : INFO : EPOCH 1 - PROGRESS: at 98.16% examples, 249080 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 22:17:25,110 : INFO : EPOCH 1: training on 5713167 raw words (4165070 effective words) took 16.7s, 248850 effective words/s\n",
      "2024-02-27 22:17:26,150 : INFO : EPOCH 2 - PROGRESS: at 5.73% examples, 233650 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 22:17:27,172 : INFO : EPOCH 2 - PROGRESS: at 11.92% examples, 242653 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 22:17:28,173 : INFO : EPOCH 2 - PROGRESS: at 18.01% examples, 244537 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 22:17:29,187 : INFO : EPOCH 2 - PROGRESS: at 23.80% examples, 243164 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 22:17:30,193 : INFO : EPOCH 2 - PROGRESS: at 29.82% examples, 244269 words/s, in_qsize 6, out_qsize 0\n",
      "2024-02-27 22:17:31,220 : INFO : EPOCH 2 - PROGRESS: at 35.74% examples, 244141 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 22:17:32,246 : INFO : EPOCH 2 - PROGRESS: at 41.99% examples, 245074 words/s, in_qsize 6, out_qsize 0\n",
      "2024-02-27 22:17:33,281 : INFO : EPOCH 2 - PROGRESS: at 48.19% examples, 245557 words/s, in_qsize 6, out_qsize 0\n",
      "2024-02-27 22:17:34,302 : INFO : EPOCH 2 - PROGRESS: at 54.55% examples, 246227 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 22:17:35,321 : INFO : EPOCH 2 - PROGRESS: at 60.66% examples, 247002 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 22:17:36,354 : INFO : EPOCH 2 - PROGRESS: at 66.71% examples, 247424 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 22:17:37,367 : INFO : EPOCH 2 - PROGRESS: at 73.11% examples, 248206 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 22:17:38,398 : INFO : EPOCH 2 - PROGRESS: at 79.62% examples, 248529 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 22:17:39,401 : INFO : EPOCH 2 - PROGRESS: at 85.31% examples, 248205 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 22:17:40,403 : INFO : EPOCH 2 - PROGRESS: at 91.21% examples, 247965 words/s, in_qsize 5, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-27 22:17:41,428 : INFO : EPOCH 2 - PROGRESS: at 96.99% examples, 247357 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 22:17:41,953 : INFO : EPOCH 2: training on 5713167 raw words (4166361 effective words) took 16.8s, 247393 effective words/s\n",
      "2024-02-27 22:17:42,980 : INFO : EPOCH 3 - PROGRESS: at 5.73% examples, 236829 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 22:17:44,019 : INFO : EPOCH 3 - PROGRESS: at 11.97% examples, 242180 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 22:17:45,050 : INFO : EPOCH 3 - PROGRESS: at 18.17% examples, 244327 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 22:17:46,075 : INFO : EPOCH 3 - PROGRESS: at 24.28% examples, 245837 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 22:17:47,099 : INFO : EPOCH 3 - PROGRESS: at 30.53% examples, 246935 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 22:17:48,107 : INFO : EPOCH 3 - PROGRESS: at 36.38% examples, 247101 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 22:17:49,114 : INFO : EPOCH 3 - PROGRESS: at 42.58% examples, 247283 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 22:17:50,154 : INFO : EPOCH 3 - PROGRESS: at 48.72% examples, 247281 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 22:17:51,167 : INFO : EPOCH 3 - PROGRESS: at 54.88% examples, 247206 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 22:17:52,189 : INFO : EPOCH 3 - PROGRESS: at 60.96% examples, 247852 words/s, in_qsize 6, out_qsize 0\n",
      "2024-02-27 22:17:53,210 : INFO : EPOCH 3 - PROGRESS: at 67.07% examples, 248421 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 22:17:54,218 : INFO : EPOCH 3 - PROGRESS: at 73.28% examples, 248589 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 22:17:55,249 : INFO : EPOCH 3 - PROGRESS: at 79.80% examples, 248894 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 22:17:56,253 : INFO : EPOCH 3 - PROGRESS: at 85.31% examples, 247992 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 22:17:57,278 : INFO : EPOCH 3 - PROGRESS: at 91.06% examples, 246909 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 22:17:58,300 : INFO : EPOCH 3 - PROGRESS: at 96.84% examples, 246411 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 22:17:58,832 : INFO : EPOCH 3: training on 5713167 raw words (4165528 effective words) took 16.9s, 246793 effective words/s\n",
      "2024-02-27 22:17:59,856 : INFO : EPOCH 4 - PROGRESS: at 5.73% examples, 237512 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 22:18:00,885 : INFO : EPOCH 4 - PROGRESS: at 11.92% examples, 243911 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 22:18:01,925 : INFO : EPOCH 4 - PROGRESS: at 18.17% examples, 244668 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 22:18:02,927 : INFO : EPOCH 4 - PROGRESS: at 23.80% examples, 242229 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 22:18:03,935 : INFO : EPOCH 4 - PROGRESS: at 29.65% examples, 241995 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 22:18:04,942 : INFO : EPOCH 4 - PROGRESS: at 35.24% examples, 240641 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 22:18:05,974 : INFO : EPOCH 4 - PROGRESS: at 41.46% examples, 241917 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 22:18:06,990 : INFO : EPOCH 4 - PROGRESS: at 47.68% examples, 243315 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 22:18:08,004 : INFO : EPOCH 4 - PROGRESS: at 54.22% examples, 245193 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 22:18:09,017 : INFO : EPOCH 4 - PROGRESS: at 60.33% examples, 246148 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 22:18:10,065 : INFO : EPOCH 4 - PROGRESS: at 66.23% examples, 245654 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 22:18:11,104 : INFO : EPOCH 4 - PROGRESS: at 72.55% examples, 246055 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 22:18:12,117 : INFO : EPOCH 4 - PROGRESS: at 78.90% examples, 246334 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 22:18:13,141 : INFO : EPOCH 4 - PROGRESS: at 85.03% examples, 246775 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 22:18:14,156 : INFO : EPOCH 4 - PROGRESS: at 91.06% examples, 246933 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 22:18:15,173 : INFO : EPOCH 4 - PROGRESS: at 96.99% examples, 246936 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 22:18:15,689 : INFO : EPOCH 4: training on 5713167 raw words (4165292 effective words) took 16.9s, 247119 effective words/s\n",
      "2024-02-27 22:18:15,690 : INFO : FastText lifecycle event {'msg': 'training on 28565835 raw words (20826345 effective words) took 84.0s, 247861 effective words/s', 'datetime': '2024-02-27T22:18:15.690849', 'gensim': '4.3.0', 'python': '3.8.17 (default, Jul  5 2023, 16:07:30) \\n[Clang 14.0.6 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'train'}\n",
      "2024-02-27 22:18:16,973 : INFO : FastText lifecycle event {'params': 'FastText<vocab=48208, vector_size=100, alpha=0.025>', 'datetime': '2024-02-27T22:18:16.973943', 'gensim': '4.3.0', 'python': '3.8.17 (default, Jul  5 2023, 16:07:30) \\n[Clang 14.0.6 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "# FastText\n",
    "from gensim.models import FastText\n",
    "fasttext_model = FastText(sentences=text, vector_size=100, window=5, min_count=5, workers=3, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score sum avec FastText: 0.8162\n",
      "Score mean avec FastText: 0.8078\n"
     ]
    }
   ],
   "source": [
    "print(\"Score sum avec FastText:\",evaluation(np.sum, fasttext_model))\n",
    "print(\"Score mean avec FastText:\",evaluation(np.mean, fasttext_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-27 22:24:51,143 : INFO : Doc2Vec lifecycle event {'params': 'Doc2Vec<dm/m,d100,n5,w5,mc5,s0.001,t3>', 'datetime': '2024-02-27T22:24:51.143073', 'gensim': '4.3.0', 'python': '3.8.17 (default, Jul  5 2023, 16:07:30) \\n[Clang 14.0.6 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'created'}\n",
      "2024-02-27 22:24:51,143 : INFO : collecting all words and their counts\n",
      "2024-02-27 22:24:51,144 : INFO : PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags\n",
      "2024-02-27 22:24:51,460 : INFO : PROGRESS: at example #10000, processed 2301366 words (7289152 words/s), 153853 word types, 0 tags\n",
      "2024-02-27 22:24:51,797 : INFO : PROGRESS: at example #20000, processed 4553558 words (6707075 words/s), 240043 word types, 0 tags\n",
      "2024-02-27 22:24:51,965 : INFO : collected 276678 word types and 25000 unique tags from a corpus of 25000 examples and 5713167 words\n",
      "2024-02-27 22:24:51,966 : INFO : Creating a fresh vocabulary\n",
      "2024-02-27 22:24:52,170 : INFO : Doc2Vec lifecycle event {'msg': 'effective_min_count=5 retains 48208 unique words (17.42% of original 276678, drops 228470)', 'datetime': '2024-02-27T22:24:52.170525', 'gensim': '4.3.0', 'python': '3.8.17 (default, Jul  5 2023, 16:07:30) \\n[Clang 14.0.6 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "2024-02-27 22:24:52,171 : INFO : Doc2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 5389596 word corpus (94.34% of original 5713167, drops 323571)', 'datetime': '2024-02-27T22:24:52.171333', 'gensim': '4.3.0', 'python': '3.8.17 (default, Jul  5 2023, 16:07:30) \\n[Clang 14.0.6 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "2024-02-27 22:24:52,382 : INFO : deleting the raw counts dictionary of 276678 items\n",
      "2024-02-27 22:24:52,387 : INFO : sample=0.001 downsamples 44 most-common words\n",
      "2024-02-27 22:24:52,388 : INFO : Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 4165010.0598832574 word corpus (77.3%% of prior 5389596)', 'datetime': '2024-02-27T22:24:52.388106', 'gensim': '4.3.0', 'python': '3.8.17 (default, Jul  5 2023, 16:07:30) \\n[Clang 14.0.6 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "2024-02-27 22:24:52,697 : INFO : estimated required memory for 48208 words and 100 dimensions: 77670400 bytes\n",
      "2024-02-27 22:24:52,698 : INFO : resetting layer weights\n",
      "2024-02-27 22:24:52,725 : INFO : Doc2Vec lifecycle event {'msg': 'training model with 3 workers on 48208 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2024-02-27T22:24:52.725955', 'gensim': '4.3.0', 'python': '3.8.17 (default, Jul  5 2023, 16:07:30) \\n[Clang 14.0.6 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'train'}\n",
      "2024-02-27 22:24:53,733 : INFO : EPOCH 0 - PROGRESS: at 25.82% examples, 1079986 words/s, in_qsize 6, out_qsize 0\n",
      "2024-02-27 22:24:54,744 : INFO : EPOCH 0 - PROGRESS: at 52.30% examples, 1083934 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 22:24:55,749 : INFO : EPOCH 0 - PROGRESS: at 79.45% examples, 1097597 words/s, in_qsize 6, out_qsize 0\n",
      "2024-02-27 22:24:56,509 : INFO : EPOCH 0: training on 5713167 raw words (4189550 effective words) took 3.8s, 1108648 effective words/s\n",
      "2024-02-27 22:24:57,514 : INFO : EPOCH 1 - PROGRESS: at 26.57% examples, 1108329 words/s, in_qsize 6, out_qsize 0\n",
      "2024-02-27 22:24:58,522 : INFO : EPOCH 1 - PROGRESS: at 53.16% examples, 1103304 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 22:24:59,524 : INFO : EPOCH 1 - PROGRESS: at 80.64% examples, 1116256 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 22:25:00,274 : INFO : EPOCH 1: training on 5713167 raw words (4189585 effective words) took 3.8s, 1113373 effective words/s\n",
      "2024-02-27 22:25:01,282 : INFO : EPOCH 2 - PROGRESS: at 26.75% examples, 1112999 words/s, in_qsize 6, out_qsize 0\n",
      "2024-02-27 22:25:02,289 : INFO : EPOCH 2 - PROGRESS: at 53.71% examples, 1113338 words/s, in_qsize 6, out_qsize 0\n",
      "2024-02-27 22:25:03,297 : INFO : EPOCH 2 - PROGRESS: at 81.47% examples, 1125682 words/s, in_qsize 6, out_qsize 0\n",
      "2024-02-27 22:25:03,982 : INFO : EPOCH 2: training on 5713167 raw words (4190603 effective words) took 3.7s, 1130908 effective words/s\n",
      "2024-02-27 22:25:04,988 : INFO : EPOCH 3 - PROGRESS: at 27.37% examples, 1142617 words/s, in_qsize 6, out_qsize 0\n",
      "2024-02-27 22:25:05,991 : INFO : EPOCH 3 - PROGRESS: at 54.69% examples, 1137165 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 22:25:06,994 : INFO : EPOCH 3 - PROGRESS: at 83.18% examples, 1153913 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 22:25:07,578 : INFO : EPOCH 3: training on 5713167 raw words (4190745 effective words) took 3.6s, 1165870 effective words/s\n",
      "2024-02-27 22:25:08,587 : INFO : EPOCH 4 - PROGRESS: at 27.37% examples, 1139764 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 22:25:09,593 : INFO : EPOCH 4 - PROGRESS: at 55.05% examples, 1140934 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 22:25:10,602 : INFO : EPOCH 4 - PROGRESS: at 82.65% examples, 1141796 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 22:25:11,238 : INFO : EPOCH 4: training on 5713167 raw words (4190635 effective words) took 3.7s, 1145664 effective words/s\n",
      "2024-02-27 22:25:11,238 : INFO : Doc2Vec lifecycle event {'msg': 'training on 28565835 raw words (20951118 effective words) took 18.5s, 1131719 effective words/s', 'datetime': '2024-02-27T22:25:11.238844', 'gensim': '4.3.0', 'python': '3.8.17 (default, Jul  5 2023, 16:07:30) \\n[Clang 14.0.6 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'train'}\n"
     ]
    }
   ],
   "source": [
    "# Doc2Vec\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "tagged_data = [TaggedDocument(words=d, tags=[i]) for i, d in enumerate(text)]\n",
    "doc2vec_model = Doc2Vec(vector_size=100, window=5, min_count=5, workers=3)\n",
    "doc2vec_model.build_vocab(tagged_data)\n",
    "doc2vec_model.train(tagged_data, total_examples=doc2vec_model.corpus_count, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score sum avec Doc2Vec: 0.7764\n",
      "Score mean avec Doc2Vec: 0.781\n"
     ]
    }
   ],
   "source": [
    "print(\"Score sum avec Doc2Vec:\",evaluation(np.sum, doc2vec_model))\n",
    "print(\"Score mean avec Doc2Vec:\",evaluation(np.mean, doc2vec_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "vscode": {
   "interpreter": {
    "hash": "902a52bcf4503a473db011f1937bdfe17613b08622219712e0110e48c4958c23"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
