{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document classification : tutorial\n",
    "\n",
    "This practical session is dedicated to NLP basic tools that will enable us to handle textual data through the **Bag-of-Words (BoW)** paradigm.\n",
    "\n",
    "\n",
    "## A. Processing chain design\n",
    "**Main steps:**\n",
    "1. Data reading \\& importation\n",
    "    - The corpus can be loaded in RAM (otherwise, you should use a *data loader* to bufferise the importation)\n",
    "    - Encoding is a big problem: you have to make sure that the data are correctly read.\n",
    "2. Data pre-processing, transformation \\& filtering\n",
    "    - remove *useless* information: figures, ponctuations, capitals, *etc*... **usefulness depends on applications! [obviously]**\n",
    "    - Segment into words (=*Tokenization*)\n",
    "    - Elimination of stop-words\n",
    "    - Stemming/lemmatization (rootization)\n",
    "    - Byte-pair encoding to find compound words (e.g. Sorbonne University, City of Paris, Prime Minister, etc...)\n",
    "3. Digital data processing (Bag-of-Words)\n",
    "    - Normalization *term-frequency* / binarization\n",
    "    - Inverse document frequency* normalization\n",
    "    - Elimination of rare words, too frequent words\n",
    "    - Construction of separability criteria to eliminate words etc...\n",
    "4. Learning a classifier\n",
    "    - Choice of the type of classifier\n",
    "    - Adjustment of the parameters of the classifier (regularization, balancing, etc...)\n",
    "\n",
    "## B. Exploitation of the processing chain\n",
    "\n",
    "This step is called the realization of a campaign of experiments: it is the key point that we want to work on in NLP this year.\n",
    "1. It is impossible to test all the combinations in relation to the above proposals... Therefore, we have to eliminate some of them.\n",
    "    - By discussing with the business experts\n",
    "    - By doing preliminary tests\n",
    "1. After this first filtering, we must:\n",
    "    - Choose an evaluation that is reliable and not too slow (cross-validation, leave-one-out, split learning/single test)\n",
    "    - Run large experiments\n",
    "        - = *grid-search\n",
    "        - parallelise on several machines\n",
    "        - know how to run on a server and disconnect\n",
    "1. Collect and analyze results\n",
    "\n",
    "\n",
    "## C. Inference & industrialization\n",
    "\n",
    "The inference is then very classical: the optimal processing chain is able to process new documents\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A-2: reprocessing & string manipulations \n",
    "\n",
    "1. Understand the examples\n",
    "1. Build a chain where each action can be activated/disactivated\n",
    "\n",
    "If you are not familia with regex, take a look at: https://docs.python.org/3/howto/regex.html\n",
    "\n",
    "**Regex** is probably the most important tool to custom your processing chain!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le chat est devant la maison, 9 rue du zoo. Le chien attend à l'intérieur.\n",
      "Son site web préféré est www.spa.fr \n"
     ]
    }
   ],
   "source": [
    "doc = 'Le chat est devant la maison, 9 rue du zoo. Le chien attend à l\\'intérieur.\\nSon site web préféré est www.spa.fr '\n",
    "print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le chat est devant la maison, 9 rue du zoo. Le chien attend à l'intérieur.\n",
      "Son site web préféré est URL \n",
      "Le chat est devant la maison,  rue du zoo. Le chien attend à l'intérieur.\n",
      "Son site web préféré est URL \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# replace url by a keywork URL\n",
    "doc = re.sub('(www|http)[\\w\\.-_]+\\.(fr|com|org)', 'URL', doc)  # note: this regex is far from perfect\n",
    "print(doc)\n",
    "# you can add some special token into the document (thus add token into the dictionary)\n",
    "\n",
    "# suppress numbers\n",
    "doc = re.sub('[0-9]+', '', doc) # remplacer une séquence de chiffres par rien\n",
    "print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
      "\r",
      "\t\n",
      "Le chat est devant la maison   rue du zoo  Le chien attend à l intérieur  Son site web préféré est URL \n"
     ]
    }
   ],
   "source": [
    "# retrieving (and suppressing) ponctuation\n",
    "import string\n",
    "\n",
    "punc = string.punctuation  \n",
    "print(punc)\n",
    "punc += '\\n\\r\\t'\n",
    "print(punc)\n",
    "doc = doc.translate(str.maketrans(punc, ' ' * len(punc)))  \n",
    "print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "le chat est devant la maison   rue du zoo  le chien attend a l interieur  son site web prefere est url \n"
     ]
    }
   ],
   "source": [
    "# suppress accent and all non normalized char\n",
    "import unicodedata\n",
    "\n",
    "doc = unicodedata.normalize('NFD', doc).encode('ascii', 'ignore').decode(\"utf-8\")\n",
    "doc = doc.lower()\n",
    "print(doc )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A-3: Bag od Words Model \n",
    "## Tokenization (=String split) & dictionary \n",
    "\n",
    "Let's tokenize !\n",
    "\n",
    "**WARNING** : we use a standard syntax to understand the process... Later, we are going to use advanced build-in functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'le': 2, 'est': 2, 'chat': 1, 'devant': 1, 'la': 1, 'maison': 1, 'rue': 1, 'du': 1, 'zoo': 1, 'chien': 1, 'attend': 1, 'a': 1, 'l': 1, 'interieur': 1, 'son': 1, 'site': 1, 'web': 1, 'prefere': 1, 'url': 1})\n"
     ]
    }
   ],
   "source": [
    "# word list\n",
    "words = doc.split() # optional args to choose the splitting chars\n",
    "\n",
    "# counting\n",
    "from collections import Counter\n",
    "\n",
    "dico = Counter(words)\n",
    "print(dico)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'le': 0, 'chat': 1, 'est': 2, 'devant': 3, 'la': 4, 'maison': 5, 'rue': 6, 'du': 7, 'zoo': 8, 'chien': 9, 'attend': 10, 'a': 11, 'l': 12, 'interieur': 13, 'son': 14, 'site': 15, 'web': 16, 'prefere': 17, 'url': 18}\n",
      "index of word chat: 1\n"
     ]
    }
   ],
   "source": [
    "# dictionary = mapping function : word => index of the word in the dictionary\n",
    "\n",
    "trans = dict(zip(list(dico.keys()), np.arange(len(dico)).tolist()))\n",
    "print(trans)\n",
    "\n",
    "# test\n",
    "print('index of word chat:',trans['chat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "# Building the vector associated to the sentence\n",
    "\n",
    "d = np.zeros(len(trans))\n",
    "for m in words:\n",
    "    d[trans[m]] += 1\n",
    "    \n",
    "print(d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t2.0\n",
      "  (0, 1)\t1.0\n",
      "  (0, 2)\t2.0\n",
      "  (0, 3)\t1.0\n",
      "  (0, 4)\t1.0\n",
      "  (0, 5)\t1.0\n",
      "  (0, 6)\t1.0\n",
      "  (0, 7)\t1.0\n",
      "  (0, 8)\t1.0\n",
      "  (0, 9)\t1.0\n",
      "  (0, 10)\t1.0\n",
      "  (0, 11)\t1.0\n",
      "  (0, 12)\t1.0\n",
      "  (0, 13)\t1.0\n",
      "  (0, 14)\t1.0\n",
      "  (0, 15)\t1.0\n",
      "  (0, 16)\t1.0\n",
      "  (0, 17)\t1.0\n",
      "  (0, 18)\t1.0\n"
     ]
    }
   ],
   "source": [
    "# switching to a sparse matrix to save some space\n",
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "ds = coo_matrix(d)\n",
    "\n",
    "print(ds)   # obviously, the current vector is full of data\n",
    "            # in the future with real data, vectors will be full of 0!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced framwork to compute bag of words \n",
    "\n",
    "The function ```sklearn.feature_extraction.text.CountVectorizer``` can perform all previous steps... And much more through (many) optional arguments\n",
    "\n",
    "1. Documentation <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\"> link </a>\n",
    "    - test on toy data\n",
    "    - check your ability to exploit the dictionary \n",
    "        - retrieve the index of a word\n",
    "        - retrieve the word corresponding to an index\n",
    "    - check the filtering options (rare words etc)\n",
    "\n",
    "1. Try to extract N-grams (bigram and trigram) (cf documentation)\n",
    "\n",
    "1. it is critical to distinguish dictionary building and dictionary exploitation. (1) At the pre-processing step, we increase the vocabulary size and filter useless words... (2) The training step consists in finding the coefficients associated with each word. (3) In the inference step, the dictionary should be fixed (otherwise, the coefficients become meaningless).\n",
    "    - split your dataset into two parts\n",
    "    - constituer le dictionnaire sur les données d'apprentissage\n",
    "    - appliquer sur les données de test et vérifier que les indices d'un même mot sont bien identiques entre l'apprentissage et le test.\n",
    "    \n",
    "**WARNING** do not confuse *fit_transform* = building + use with *transform* (use only, dictionary fixed). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and' 'document' 'first' 'is' 'one' 'second' 'the' 'third' 'this']\n",
      "[[0 1 1 1 0 0 1 0 1]\n",
      " [0 2 0 1 0 1 1 0 1]\n",
      " [1 0 0 1 1 0 1 1 1]\n",
      " [0 1 1 1 0 0 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = [\\\n",
    "     'This is the first document.',\\\n",
    "     'This document is the second document.',\\\n",
    "     'And this is the third one.',\\\n",
    "     'Is this the first document?',\\\n",
    " ]\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus) # Output is a sparse matrix\n",
    "print(vectorizer.get_feature_names_out())\n",
    "\n",
    "print(X.toarray()) # Convert to numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and this' 'document is' 'first document' 'is the' 'is this'\n",
      " 'second document' 'the first' 'the second' 'the third' 'third one'\n",
      " 'this document' 'this is' 'this the']\n",
      "[[0 0 1 1 0 0 1 0 0 0 0 1 0]\n",
      " [0 1 0 1 0 1 0 1 0 0 1 0 0]\n",
      " [1 0 0 1 0 0 0 0 1 1 0 1 0]\n",
      " [0 0 1 0 1 0 1 0 0 0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "vectorizer2 = CountVectorizer(analyzer='word', ngram_range=(2, 2))\n",
    "X2 = vectorizer2.fit_transform(corpus)\n",
    "print(vectorizer2.get_feature_names_out())\n",
    "print(X2.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and this is' 'document is the' 'is the first' 'is the second'\n",
      " 'is the third' 'is this the' 'the first document' 'the second document'\n",
      " 'the third one' 'this document is' 'this is the' 'this the first']\n",
      "[[0 0 1 0 0 0 1 0 0 0 1 0]\n",
      " [0 1 0 1 0 0 0 1 0 1 0 0]\n",
      " [1 0 0 0 1 0 0 0 1 0 1 0]\n",
      " [0 0 0 0 0 1 1 0 0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "vectorizer3 = CountVectorizer(analyzer='word', ngram_range=(3, 3))\n",
    "X3 = vectorizer3.fit_transform(corpus)\n",
    "print(vectorizer3.get_feature_names_out())\n",
    "print(X3.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial on classifiers\n",
    "\n",
    "scikit learn is object oriented: that means you will be able to switch easily from one classifer to another. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.30401786  1.90279408]\n",
      " [ 0.63265843  2.93729542]\n",
      " [ 2.76778387  1.91448594]\n",
      " [ 1.50053092  2.696296  ]\n",
      " [-0.08841403  2.05624547]\n",
      " [ 2.34902562  1.6410274 ]\n",
      " [ 2.34333685  3.21297748]\n",
      " [ 3.08505228  2.13039473]\n",
      " [ 0.590156    0.61377153]\n",
      " [ 1.29115441  0.87539085]\n",
      " [ 2.58846509  1.5371556 ]\n",
      " [ 2.77277866  0.60312622]\n",
      " [ 2.0830899   2.07902099]\n",
      " [ 2.20984011  2.44184205]\n",
      " [ 1.99031806  1.64426116]\n",
      " [ 0.35245622  2.69868354]\n",
      " [ 0.49877792  1.22909429]\n",
      " [ 2.12889797  2.21902836]\n",
      " [ 2.29786786  1.82182205]\n",
      " [ 0.9428278   2.69654855]\n",
      " [ 1.98484451  3.79259111]\n",
      " [ 3.51611149  2.56398321]\n",
      " [ 2.73596981  2.37965754]\n",
      " [ 2.01417633  2.08003924]\n",
      " [ 2.49702584  0.82140068]\n",
      " [ 2.56609721  1.95346316]\n",
      " [ 2.36539625  2.8313386 ]\n",
      " [ 2.11530787  2.66550989]\n",
      " [ 2.40746088  2.40304531]\n",
      " [ 1.27594484  0.82586443]\n",
      " [ 2.60805551  0.22516642]\n",
      " [ 1.87681137  1.59506767]\n",
      " [ 0.90783061  2.02457559]\n",
      " [ 1.75669738  3.1352835 ]\n",
      " [ 2.09521446  2.26283486]\n",
      " [ 3.43713491  2.88184294]\n",
      " [ 0.5619402   1.52901451]\n",
      " [ 2.5558579   3.37633518]\n",
      " [ 1.43690868  2.56909571]\n",
      " [ 2.04518242  2.42817257]\n",
      " [ 3.14792225  1.54551683]\n",
      " [ 2.94128193  1.36321665]\n",
      " [ 1.82824522  2.41891277]\n",
      " [ 3.27354133  3.02921011]\n",
      " [ 2.61682725  2.17134281]\n",
      " [ 2.84193472  1.34175817]\n",
      " [ 2.40344576  1.75926174]\n",
      " [ 0.41375612  2.30442902]\n",
      " [ 1.56556674  1.6169749 ]\n",
      " [ 1.15704476  2.70178442]\n",
      " [-0.55015865 -2.0129072 ]\n",
      " [-2.45614297 -2.96640781]\n",
      " [-2.09473326 -2.74072475]\n",
      " [-2.54731381 -1.54099345]\n",
      " [-3.35350561 -1.96808391]\n",
      " [-2.79226858 -3.42211433]\n",
      " [-1.71125868 -2.45527133]\n",
      " [-0.99120473 -1.21517743]\n",
      " [-2.31511444 -1.79002058]\n",
      " [-4.48902924 -1.79225461]\n",
      " [-3.67320599 -0.51933617]\n",
      " [-2.57248183 -3.14451797]\n",
      " [-0.34528695 -0.42510034]\n",
      " [-1.8321246  -3.15194338]\n",
      " [-2.02809225 -0.20980679]\n",
      " [-0.70458144 -0.44236714]\n",
      " [-1.88021165 -2.74548363]\n",
      " [-1.70501581 -2.51537389]\n",
      " [-3.16476998 -1.98982404]\n",
      " [-1.67194164 -0.86755716]\n",
      " [-1.09948334 -2.36524685]\n",
      " [-2.34482458 -2.44503327]\n",
      " [-3.51771301 -1.42340784]\n",
      " [-0.19990292 -1.09992985]\n",
      " [-0.69029124 -2.16906078]\n",
      " [-3.08408535 -3.31770382]\n",
      " [-0.76429189 -1.26716322]\n",
      " [-2.92353625 -4.24853474]\n",
      " [-1.88874434 -2.67278741]\n",
      " [-1.58846125 -1.67559478]\n",
      " [-4.86308085 -1.87710102]\n",
      " [-1.27939176 -2.18712819]\n",
      " [-2.43581521 -1.16068986]\n",
      " [-2.24273384 -1.78518713]\n",
      " [-3.39739168 -2.15976654]\n",
      " [-1.11665188 -1.07605975]\n",
      " [-2.98933648 -0.89560206]\n",
      " [-2.47085937 -1.44853127]\n",
      " [-2.92056855 -2.31831692]\n",
      " [-2.36870606 -1.09711448]\n",
      " [-2.4662582  -2.66470046]\n",
      " [-0.89840083 -0.62099267]\n",
      " [-1.14021734 -2.83570332]\n",
      " [-3.13942972 -1.00980559]\n",
      " [-2.97074235 -1.55339406]\n",
      " [-1.88006223 -5.2017845 ]\n",
      " [-0.96378132 -1.06703656]\n",
      " [-2.13010937 -2.89383472]\n",
      " [-2.40947081 -1.2860771 ]\n",
      " [-2.88703391 -2.75010315]]\n",
      "LinearSVC()\n",
      "prediction: [1]\n",
      "classifier: [[0.67704828 0.70919981]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\miniconda3\\Lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sklearn.naive_bayes as nb\n",
    "from sklearn import svm\n",
    "from sklearn import linear_model as lin\n",
    "\n",
    "# very basic data (once understood, use textual corpus instead)\n",
    "N = 100\n",
    "X = np.random.randn(N,2) \n",
    "X[:int(N/2), :] += 2\n",
    "X[int(N/2):, :] -= 2\n",
    "y = np.array([1]*int(N/2) + [-1]*int(N/2))\n",
    "print(X)\n",
    "# Linear SVM \n",
    "clf1 = svm.LinearSVC()\n",
    "# Naive Bayes\n",
    "clf2 = nb.MultinomialNB()\n",
    "# logistic regression\n",
    "clf = lin.LogisticRegression()\n",
    "\n",
    "# training\n",
    "clf1.fit(X, y)  \n",
    "yhat = clf1.predict([[2., 2.]]) # use on new data\n",
    "print(clf1)\n",
    "print(\"prediction:\",yhat)\n",
    "print(\"classifier:\",clf1.coef_) # retrieve the coefs from inside the object (cf doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Negative values in data passed to MultinomialNB (input X)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# training\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mclf2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m  \n\u001b[0;32m      3\u001b[0m yhat \u001b[38;5;241m=\u001b[39m clf2\u001b[38;5;241m.\u001b[39mpredict([[\u001b[38;5;241m2.\u001b[39m, \u001b[38;5;241m2.\u001b[39m]]) \u001b[38;5;66;03m# use on new data\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(clf2)\n",
      "File \u001b[1;32m~\\miniconda3\\Lib\\site-packages\\sklearn\\base.py:1351\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1344\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1346\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1347\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1348\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1349\u001b[0m     )\n\u001b[0;32m   1350\u001b[0m ):\n\u001b[1;32m-> 1351\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\Lib\\site-packages\\sklearn\\naive_bayes.py:759\u001b[0m, in \u001b[0;36m_BaseDiscreteNB.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    757\u001b[0m n_classes \u001b[38;5;241m=\u001b[39m Y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    758\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_counters(n_classes, n_features)\n\u001b[1;32m--> 759\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    760\u001b[0m alpha \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_alpha()\n\u001b[0;32m    761\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_feature_log_prob(alpha)\n",
      "File \u001b[1;32m~\\miniconda3\\Lib\\site-packages\\sklearn\\naive_bayes.py:881\u001b[0m, in \u001b[0;36mMultinomialNB._count\u001b[1;34m(self, X, Y)\u001b[0m\n\u001b[0;32m    879\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_count\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, Y):\n\u001b[0;32m    880\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Count and smooth feature occurrences.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 881\u001b[0m     \u001b[43mcheck_non_negative\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMultinomialNB (input X)\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    882\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_count_ \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m safe_sparse_dot(Y\u001b[38;5;241m.\u001b[39mT, X)\n\u001b[0;32m    883\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_count_ \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m Y\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32m~\\miniconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1572\u001b[0m, in \u001b[0;36mcheck_non_negative\u001b[1;34m(X, whom)\u001b[0m\n\u001b[0;32m   1569\u001b[0m     X_min \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mmin(X)\n\u001b[0;32m   1571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m X_min \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 1572\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNegative values in data passed to \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m whom)\n",
      "\u001b[1;31mValueError\u001b[0m: Negative values in data passed to MultinomialNB (input X)"
     ]
    }
   ],
   "source": [
    "# training\n",
    "clf2.fit(X, y)  \n",
    "yhat = clf2.predict([[2., 2.]]) # use on new data\n",
    "print(clf2)\n",
    "print(\"prediction:\",yhat)\n",
    "print(\"classifier:\",clf2.coef_) # retrieve the coefs from inside the object (cf doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression()\n",
      "prediction: [1]\n",
      "classifier: [[1.34688437 1.54303109]]\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "clf.fit(X, y)  \n",
    "yhat = clf.predict([[2., 2.]]) # use on new data\n",
    "print(clf)\n",
    "print(\"prediction:\",yhat)\n",
    "print(\"classifier:\",clf.coef_) # retrieve the coefs from inside the object (cf doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "# Solution 1 to evaluate: cross validation\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# everything is inside :)\n",
    "# possible to parallelize with n_jobs\n",
    "scores = cross_val_score( clf, X, y, cv=20)\n",
    "\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1 -1  1 -1 -1 -1  1 -1 -1 -1 -1 -1 -1  1  1  1  1  1  1  1  1 -1  1 -1\n",
      "  1  1  1 -1 -1 -1 -1 -1 -1 -1 -1  1  1 -1 -1 -1]\n"
     ]
    }
   ],
   "source": [
    "# Solution 2: only one split between train/test\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# with a seed to enhence reproducibility\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.4, random_state=0) \n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Application \n",
    "yhat = clf.predict(X_test)\n",
    "print(yhat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48\n",
      " 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72\n",
      " 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96\n",
      " 97 98 99] [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24]\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72\n",
      " 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96\n",
      " 97 98 99] [25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48\n",
      " 49]\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96\n",
      " 97 98 99] [50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73\n",
      " 74]\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74] [75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98\n",
      " 99]\n"
     ]
    }
   ],
   "source": [
    "# Solution 3: explicit cross validation\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits=4)\n",
    "for train, test in kf.split(X):\n",
    "    print(\"%s %s\" % (train, test))\n",
    "    X_train = X[train]\n",
    "    y_train = y[train]\n",
    "    X_test  = X[test]\n",
    "    y_test  = y[test]\n",
    "    # training\n",
    "    # evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# let's move on real data\n",
    "# WARMUP (just read and run)\n",
    "## Step 1: Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the happy bastard's quick movie review \n",
      "damn that y2k bug . \n",
      "it's got a head start in this movie starring jamie lee curtis and another baldwin brother ( william this time ) in a story regarding a crew of a tugboat that comes across a deserted russian tech ship that has a strangeness to it when they kick the power back on . \n",
      "little do they know the power within . . . \n",
      "going for the gore and bringing on a few action sequences here and there , virus still feels very empty , like a movie going for all flash and no substance . \n",
      "we don't know why the crew was really out in the middle of nowhere , we don't know the origin of what took over the ship ( just that a big pink flashy thing hit the mir ) , and , of course , we don't know why donald sutherland is stumbling around drunkenly throughout . \n",
      "here , it's just \" hey , let's chase these people around with some robots \" . \n",
      "the acting is below average , even from the likes of curtis . \n",
      "you're more likely to get a kick out of her work in halloween h20 . \n",
      "sutherland is wasted and baldwin , well , he's acting like a baldwin , of course . \n",
      "the real star here are stan winston's robot design , some schnazzy cgi , and the occasional good gore shot , like picking into someone's brain . \n",
      "so , if robots and body parts really turn you on , here's your movie . \n",
      "otherwise , it's pretty much a sunken ship of a movie . \n",
      "\n",
      "0\n",
      "-----------------\n",
      "so ask yourself what \" 8mm \" ( \" eight millimeter \" ) is really all about . \n",
      "is it about a wholesome surveillance man who loses sight of his values after becoming enmeshed in the seedy , sleazy underworld of hardcore pornography ? \n",
      "is it about the business itself , how , bubbling just beneath the surface of big-town americana , there's a sordid world of sick and depraved people who won't necessarily stop short of murder in order to satisfy their sick and twisted desires ? \n",
      "or is it about those who can , those who are in a position to influence the making of the kinds of films sick and demented people want to see ? \n",
      "i'm not talking about snuff films , supposed \" documentaries \" of victims being brutalized and killed on camera . \n",
      "i'm talking about films like \" 8mm \" and its director , joel schumacher . \n",
      "with a recent run of big budget movies to his credit-- \" batman & robin , \" \" a time to kill , \" \" batman forever , \" \" the client \" --schumacher certainly has that kind of influence . \n",
      "is \" 8mm \" something you really want to see ? \n",
      "probably not . \n",
      "the first two-thirds of \" 8mm \" unwind as a fairly conventional missing persons drama , albeit with a particularly unsavory core . \n",
      "then , as it's been threatening all along , the film explodes into violence . \n",
      "and just when you think it's finally over , schumacher tags on a ridiculous self-righteous finale that drags the whole unpleasant experience down even further . \n",
      "trust me . \n",
      "there are better ways to waste two hours of your life . \n",
      "nicolas ' \" snake eyes \" ' cage plays private investigator tom welles who is hired by a wealthy philadelphia widow to determine whether a reel of film found in her late husband's safe documents a young girl's murder . \n",
      "welles goes about his assignment rather matter-of-factly , and the pieces of the puzzle fall into place rather neatly , almost as if you don't need any specialized skills or training to do this . \n",
      "welles certainly makes it look easy . \n",
      "and cops , obviously , never look in toilet tanks for clues . \n",
      "the deeper welles digs into his investigation the more obsessed he becomes , like george c . scott in paul schrader's \" hardcore . \" \n",
      "occasionally , a little flickering sound whirs in his head like sprockets winding through a film projector , reminding him of his unpleasant task . \n",
      "there are hints that this is taking its toll on his lovely wife , played by catherine keener , who is frustrated by her husband spending all of his time in cleveland rather than in their ugly split-level home in harrisburg , pa . \n",
      " \" 8mm \" doesn't condemn or condone its subject matter , it just exploits it . \n",
      "the irony , of course , is that schumacher and \" seven \" scribe andrew kevin walker's vision of life in the snuff lane is limited by what they can show in an r-rated , first-run hollywood product . \n",
      "so we only see snippets of snuff , and a lot more footage of nicolas cage covering his face in horror . \n",
      "later it's the turn of joaquin phoenix ( who's quite good and by far the film's most interesting character as adult bookstore flunky max california ) to cover his face as the horrid thing is screened over and over again . \n",
      "all this to get to the familiar yet offensive \" revelation \" that sexual deviants are not , indeed , monsters but everyday people like you and me . \n",
      "neither super nor standard , \" 8mm \" is shocking only in its banality . \n",
      "\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "\n",
    "def load_movies(path2data): # 1 classe par répertoire\n",
    "    alltxts = [] # init vide\n",
    "    labs = []\n",
    "    cpt = 0\n",
    "    for cl in os.listdir(path2data): # parcours des fichiers d'un répertoire\n",
    "        for f in os.listdir(path2data+cl):\n",
    "            txt = open(path2data+cl+'/'+f).read()\n",
    "            alltxts.append(txt)\n",
    "            labs.append(cpt)\n",
    "        cpt+=1 # chg répertoire = cht classe\n",
    "        \n",
    "    return alltxts,labs\n",
    "\n",
    "path = \"./datasets/movies/movies1000/\"\n",
    "alltxts,alllabs = load_movies(path)\n",
    "print(alltxts[1])\n",
    "print(alllabs[1])\n",
    "\n",
    "print(\"-----------------\")\n",
    "print(alltxts[6])\n",
    "print(alllabs[6])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the data, we need to vectorize the text so it can be used by classifiers.\n",
    "Different methods exists to vectorize text. Here we use a [bag of word](https://en.wikipedia.org/wiki/Bag-of-words_model) approach:\n",
    "\n",
    ">  In the bag of word model, a text (such as a sentence or a document) is represented as the bag (multiset) of its words, disregarding grammar and even word order but keeping multiplicity.\n",
    "\n",
    "In other words, each text becomes a (sparse) vector which codes for its words. With the following function from scikit-learn, it is straightforward to get a bag of word from raw texts: [CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)\n",
    "\n",
    ">Convert a collection of text documents to a matrix of token counts\n",
    "This implementation produces a sparse representation of the counts using scipy.sparse.csr_matrix.\n",
    "If you do not provide an a-priori dictionary and you do not use an analyzer that does some kind of feature selection then the number of features will be equal to the vocabulary size found by analyzing the data. [USER GUIDE](http://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "\n",
    "vectorizer1 = CountVectorizer()\n",
    "vectorizer2 = TfidfVectorizer()\n",
    "X = vectorizer1.fit_transform(alltxts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 39659)\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(vectorizer1.get_feature_names_out()[70000:70500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 39659)\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "X = vectorizer2.fit_transform(alltxts)\n",
    "print(X.shape)\n",
    "print(vectorizer2.get_feature_names_out()[70000:70500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------\n",
    "\n",
    "# EXERCISES (now it's your turn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To further improve performances, we can try to improve the simple bag of word model as used above to address some issues **=> BoW variants:**\n",
    " \n",
    "- **(a) Pre-prcessing, clean text:** punctuation, UPPER CASE, numbers... Do cleaning bring improvements?\n",
    "- **(b) Dictionary processing**: stop words (existing/corpus-specific list), frequent vs infrequent words, max number of words- \n",
    "- **(c) Binary vectors**\n",
    "- **(d) TF-IDF**\n",
    "- **(e) N-grams**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a) Clean text ?**\n",
    "- **strip_accents:** {‘ascii’, ‘unicode’, None}\n",
    "Remove accents and perform other character normalization during the preprocessing step. ‘ascii’ is a fast method that only works on characters that have an direct ASCII mapping. ‘unicode’ is a slightly slower method that works on any characters. None (default) does nothing.\n",
    "Both ‘ascii’ and ‘unicode’ use NFKD normalization from unicodedata.normalize.\n",
    "\n",
    "- **lowercase:** boolean, True by default\n",
    "Convert all characters to lowercase before tokenizing.\n",
    "\n",
    "- **preprocessor:** callable or None (default)\n",
    "Override the preprocessing (string transformation) stage while preserving the tokenizing and n-grams generation steps.\n",
    "\n",
    "    - **token_pattern:** string. Regular expression denoting what constitutes a “token”, only used if analyzer == 'word'. The default regexp select tokens of 2 or more alphanumeric characters (punctuation is completely ignored and always treated as a token separator).\n",
    "    - **`Prepocessor`**: argument takes a function which processes text directly. This way it becomes easy to do \"fancy\" things like:\n",
    "        - [part of speech tagging](https://en.wikipedia.org/wiki/Part-of-speech_tagging)\n",
    "        - [stemming](https://en.wikipedia.org/wiki/Stemming) \n",
    "        - To do both, you can use [NLTK tagger](http://www.nltk.org/api/nltk.tag.html) and [NLTK stemmer](http://www.nltk.org/api/nltk.stem.html#module-nltk.stem).\n",
    "\n",
    "- **[Byte-pair encoding](https://www.freecodecamp.org/news/evolution-of-tokenization/)** to find compound words (e.g. Sorbonne University, City of Paris, Prime Minister, etc...)\n",
    "\n",
    "[see CountVectorizer for full doc](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "reg = \"\\b[^\\W]\\b\" #matches word with characters \n",
    "\n",
    "# 1) Try removing punctuation or putting text to lower case (maybe use a regex)\n",
    "# 2) Try \"Stemming\" - \"pos-tagging\" the text\n",
    "\n",
    "def preprocess(text):\n",
    "    \"\"\"\n",
    "    Transforms text to remove unwanted bits.\n",
    "    \"\"\"\n",
    "    punc = string.punctuation \n",
    "    punc += '\\n\\r\\t'\n",
    "    text2 = text.translate(str.maketrans(punc, ' ' * len(punc)))\n",
    "    return text2.replace(\".\",\" \") # This function is only taking care of dots, what about !:,?+-&*%\n",
    "\n",
    "vectorizer = CountVectorizer(lowercase=True,strip_accents='ascii',preprocessor=preprocess)\n",
    "X = vectorizer.fit_transform(alltxts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b) Dictionary processing**: If we visualize the word frequency distribution we see that a few words (roughly 20) appear a lot more than the others. These words are often refered to as **stop words**. Would remove them improve accuracy ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(',', 77717), ('the', 68368), ('.', 65876), ('a', 37123), ('and', 33726), ('of', 33698), ('to', 31471), ('is', 25017), ('in', 20012), ('\"', 17612), ('that', 14768), (')', 11781), ('(', 11664), ('it', 10546), ('as', 10422), ('with', 10407), ('for', 9427), ('his', 9008), ('film', 8843), ('this', 7965)]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's plot the count of the 1000 most used words:\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "wc = Counter()\n",
    "for text in alltxts:\n",
    "    wc.update(text.split(\" \"))\n",
    "    \n",
    "freq = [f for w,f in wc.most_common(1000)]\n",
    "\n",
    "plt.plot(freq[:1000])\n",
    "print(wc.most_common(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's remove stopwords:** english stop words: direclty on sklearn \n",
    "\n",
    "- **stop_words:** string {‘english’}, list, or None (default)\n",
    "    - If ‘english’, a built-in stop word list for English is used. There are several known issues with ‘english’ and you should consider an alternative (see Using stop words).\n",
    "    - If a list, that list is assumed to contain stop words, all of which will be removed from the resulting tokens. Only applies if analyzer == 'word'.\n",
    "    - If None, no stop words will be used. max_df can be set to a value in the range [0.7, 1.0) to automatically detect and filter stop words based on intra corpus document frequency of terms.\n",
    "\n",
    "[see CountVectorizer for full doc](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(stop_words='english') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Build your own list**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CountVectorizer can take a list of stop words as argument.\n",
    "# Build or download a list of stop word (from NLTK for exemple)\n",
    "\n",
    "stop_words = [\"the\", \"a\", \"and\"] #Make a better list\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words=stop_words)\n",
    "#X = vectorizer.fit_transform(alltxts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stopwords from other languages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "# French stop words: nltk\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "final_stopwords_list = stopwords.words('english') + stopwords.words('french')\n",
    "#print(final_stopwords_list)\n",
    "vectorizer = CountVectorizer(stop_words=final_stopwords_list)\n",
    "#vectorizer = TfidfVectorizer(stop_words=final_stopwords_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b) Dictionary processing, restricting vocabulary size: corpus-specific stopwords** => max_df + suppress rare words (min_df) + max_features \n",
    "\n",
    "- **max_df:** float in range [0.0, 1.0] or int, default=1.0\n",
    "When building the vocabulary ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words). If float, the parameter represents a proportion of documents, integer absolute counts. This parameter is ignored if vocabulary is not None.\n",
    "\n",
    "- **min_df:** float in range [0.0, 1.0] or int, default=1\n",
    "When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold. This value is also called cut-off in the literature. If float, the parameter represents a proportion of documents, integer absolute counts. This parameter is ignored if vocabulary is not None.\n",
    "\n",
    "- **max_features:** int or None, default=None\n",
    "If not None, build a vocabulary that only consider the top max_features ordered by term frequency across the corpus.\n",
    "This parameter is ignored if vocabulary is not None.\n",
    "\n",
    "[see CountVectorizer for full doc](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "# max_df: float (ratio) / integer(number) of document above which we remove the word)\n",
    "# min_df: float (ratio) / integer(number) of document under which we remove the word)\n",
    "\n",
    "min_df=5             \n",
    "max_df=0.5\n",
    "max_features=10000\n",
    "vectorizer = CountVectorizer(max_df=max_df,min_df=min_df,max_features=max_features) #try out some values\n",
    "X = vectorizer.fit_transform(alltxts)\n",
    "#What is the dictionnary size now ?\n",
    "dic_size = X.shape[1]###\n",
    "print(dic_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c) : Binary BoW**: instead of word counts, the bag of word vector can only represent used word.\n",
    "\n",
    "- **binary:** boolean, default=False\n",
    "If True, all non zero counts are set to 1. This is useful for discrete probabilistic models that model binary events rather than integer counts.\n",
    "\n",
    "[see CountVectorizer for full doc](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d) TF-IDF**: words can also be weighted by importance. $\\mbox{Corpus : } C = \\{\\mathbf d_{1},\\ldots,\\mathbf d_{|C|}\\}$, vocabulary $V = \\{\\mathbf w_{1},\\ldots,\\mathbf w_{|V|}\\}$: \n",
    "- **$d_{ik}^{(tf)}$ term frequency** for word $w_k$ in document $d_i$, s.t. $\\sum\\limits_{k=1}^{|V|}d_{ik}^{(tf)}=1$\n",
    "- **$\\mathrm{df_{k}}$ document frequency** $\\mathrm{df_{k}} =  \\frac{|\\{\\mathbf d: t_{k} \\in \\mathbf d\\}|}{|C|}$\n",
    "\n",
    "$$ d_{ik}^{(tfidf)} = d_{ik}^{(tf)}   \\log \\frac{1}{\\mathrm{df_{k}}} $$\n",
    "\n",
    "\n",
    "## [TfidfVectorizer from scikit can be directly used](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer): experiment it!\n",
    "\n",
    "**Main paramters:**\n",
    "- **use_idf:** boolean, default=True. \n",
    "- **smooth_idf:** Smooth idf weights, default=True. Adds one to document frequencies, as if an extra document was seen containing every term in the collection exactly once. Prevents zero divisions.\n",
    "- **sublinear_tf:** boolean, default=bool, default=False. Apply sublinear tf scaling, i.e. replace $d_{ik}^{(tf)}$ with $1 + log(d_{ik}^{(tf)})$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "use_idf=True          \n",
    "smooth_idf=True      \n",
    "sublinear_tf=False    \n",
    "\n",
    "vectorizer = TfidfVectorizer(use_idf= use_idf, smooth_idf=smooth_idf, sublinear_tf=sublinear_tf)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(e) n-grams**: instead of words, consider n-uplets of words as tokens\n",
    "\n",
    "- **ngram_range:** tuple (min_n, max_n)\n",
    "The lower and upper boundary of the range of n-values for different n-grams to be extracted. All values of n such that min_n <= n <= max_n will be used.\n",
    "\n",
    "- **analyzer:** string, {‘word’, ‘char’, ‘char_wb’} or callable\n",
    "Whether the feature should be made of word or character n-grams. Option ‘char_wb’ creates character n-grams only from text inside word boundaries; n-grams at the edges of words are padded with space.\n",
    "If a callable is passed it is used to extract the sequence of features out of the raw, unprocessed input.\n",
    "\n",
    "[see CountVectorizer for full doc](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "538909\n"
     ]
    }
   ],
   "source": [
    "ngram_range = (1,2) # unigrams and bigrams\n",
    "vectorizer = CountVectorizer(ngram_range=ngram_range,analyzer='word') # Maybe 2-grams or 3-grams bring improvements ?\n",
    "X = vectorizer.fit_transform(alltxts)\n",
    "#What is the dictionnary size now ?\n",
    "dic_size = X.shape[1]###\n",
    "print(dic_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Classifiers\n",
    "\n",
    "Once we have vectorized data, we can use them to train statistical classifiers.\n",
    "\n",
    "Here, we propose to use three classic options:\n",
    "\n",
    "- Naïve bayes\n",
    "- Logistic Regression\n",
    "- SVM\n",
    "\n",
    "\n",
    "We fit each model below with default parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train / test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1600, 39659)\n",
      "(400, 39659)\n",
      "1600\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#vectorizer = CountVectorizer()\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "X = vectorizer.fit_transform(alltxts)\n",
    "\n",
    "rs=10\n",
    "[X_train, X_test, y_train, y_test]  = train_test_split(X, alllabs, test_size=0.2, random_state=rs, shuffle=True)\n",
    "\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(len(y_train))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We train each ML model on the train set an we evaluate the accuracy on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naïve Bayes accuracy train=0.96625, accuracy test=0.81\n",
      "Logistic Regression accuracy train=1.0, accuracy test=0.8325\n",
      "SVM accurac ytrain=0.999375, accuracy test=0.83\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\miniconda3\\Lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "#Naïve Bayes\n",
    "nb_clf = MultinomialNB()\n",
    "nb_clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "#Logistic Regression\n",
    "t = 1e-8\n",
    "C=100.0\n",
    "lr_clf = LogisticRegression(random_state=0, solver='liblinear',max_iter=100, tol=t, C=C)\n",
    "lr_clf.fit(X_train, y_train)\n",
    "\n",
    "#Linear SVM\n",
    "svm_clf = LinearSVC(random_state=0)\n",
    "svm_clf.fit(X_train, y_train)\n",
    "\n",
    "pred_nbt = nb_clf.predict(X_train)\n",
    "pred_lrt = lr_clf.predict(X_train)\n",
    "pred_svmt = svm_clf.predict(X_train)\n",
    "\n",
    "pred_nb = nb_clf.predict(X_test)\n",
    "pred_lr = lr_clf.predict(X_test)\n",
    "pred_svm = svm_clf.predict(X_test)\n",
    "\n",
    "\n",
    "print(f\"Naïve Bayes accuracy train={accuracy_score(y_train, pred_nbt)}, accuracy test={accuracy_score(y_test, pred_nb)}\")\n",
    "print(f\"Logistic Regression accuracy train={accuracy_score(y_train, pred_lrt)}, accuracy test={accuracy_score(y_test, pred_lr)}\")\n",
    "print(f\"SVM accurac ytrain={accuracy_score(y_train, pred_svmt)}, accuracy test={accuracy_score(y_test, pred_svm)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer the following questions\n",
    "\n",
    "- What is the most effective pre-processing ?\n",
    "- Which model is the most accurate ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving classifier performances\n",
    "- **Using regularization** if the number of training sample is small wrt BoW size\n",
    "- **Using relancing** if the number examples across class is inbalanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Models : Logistic Regression & SVM\n",
    "\n",
    "\n",
    "For linear models, we can look at feature coefficients:\n",
    " [Logistic Regression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)\n",
    " \n",
    " \t\n",
    "- **coef_:** array, shape (1, n_features) or (n_classes, n_features). Coefficient of the features in the decision function. coef_ is of shape (1, n_features) when the given problem is binary. In particular, when multi_class=’multinomial’, coef_ corresponds to outcome 1 (True) and -coef_ corresponds to outcome 0 (False)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression\n",
    "k = 50 # we want the 50 most negative and positive words\n",
    "feat = lr_clf.coef_[0] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM\n",
    "k = 50 # we want the 50 most negative and positive words\n",
    "feat = svm_clf.coef_[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
