{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP & representation learning: Neural Embeddings, Text Classification\n",
    "\n",
    "## CELIK Simay 28713301 - SOYKOK Aylin 28711545\n",
    "\n",
    "To use statistical classifiers with text, it is first necessary to vectorize the text. In the first practical session we explored the **Bag of Word (BoW)** model. \n",
    "\n",
    "Modern **state of the art** methods uses  embeddings to vectorize the text before classification in order to avoid feature engineering.\n",
    "\n",
    "## [Dataset](https://thome.isir.upmc.fr/classes/RITAL/json_pol.json)\n",
    "\n",
    "\n",
    "## \"Modern\" NLP pipeline\n",
    "\n",
    "By opposition to the **bag of word** model, in the modern NLP pipeline everything is **embeddings**. Instead of encoding a text as a **sparse vector** of length $D$ (size of feature dictionnary) the goal is to encode the text in a meaningful dense vector of a small size $|e| <<< |D|$. \n",
    "\n",
    "\n",
    "The raw classification pipeline is then the following:\n",
    "\n",
    "```\n",
    "raw text ---|embedding table|-->  vectors --|Neural Net|--> class \n",
    "```\n",
    "\n",
    "\n",
    "### Using a  language model:\n",
    "\n",
    "How to tokenize the text and extract a feature dictionnary is still a manual task. To directly have meaningful embeddings, it is common to use a pre-trained language model such as `word2vec` which we explore in this practical.\n",
    "\n",
    "In this setting, the pipeline becomes the following:\n",
    "```\n",
    "      \n",
    "raw text ---|(pre-trained) Language Model|--> vectors --|classifier (or fine-tuning)|--> class \n",
    "```\n",
    "\n",
    "\n",
    "- #### Classic word embeddings\n",
    "\n",
    " - [Word2Vec](https://arxiv.org/abs/1301.3781)\n",
    " - [Glove](https://nlp.stanford.edu/projects/glove/)\n",
    "\n",
    "\n",
    "- #### bleeding edge language models techniques (see next)\n",
    "\n",
    " - [UMLFIT](https://arxiv.org/abs/1801.06146)\n",
    " - [ELMO](https://arxiv.org/abs/1802.05365)\n",
    " - [GPT](https://blog.openai.com/language-unsupervised/)\n",
    " - [BERT](https://arxiv.org/abs/1810.04805)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Goal of this session:\n",
    "\n",
    "1. Train word embeddings on training dataset\n",
    "2. Tinker with the learnt embeddings and see learnt relations\n",
    "3. Tinker with pre-trained embeddings.\n",
    "4. Use those embeddings for classification\n",
    "5. Compare different embedding models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 0: Loading data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reviews :  25000\n",
      "----> # of positive :  12500\n",
      "----> # of negative :  12500\n",
      "\n",
      "['Although credit should have been given to Dr. Seuess for stealing the story-line of \"Horton Hatches The Egg\", this was a fine film. It touched both the emotions and the intellect. Due especially to the incredible performance of seven year old Justin Henry and a script that was sympathetic to each character (and each one\\'s predicament), the thought provoking elements linger long after the tear jerking ones are over. Overall, superior acting from a solid cast, excellent directing, and a very powerful script. The right touches of humor throughout help keep a \"heavy\" subject from becoming tedious or difficult to sit through. Lastly, this film stands the test of time and seems in no way dated, decades after it was released.', 1]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "# Loading json\n",
    "file = './datasets/json_pol.json'\n",
    "with open(file,encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "\n",
    "# Quick Check\n",
    "counter = Counter((x[1] for x in data))\n",
    "print(\"Number of reviews : \", len(data))\n",
    "print(\"----> # of positive : \", counter[1])\n",
    "print(\"----> # of negative : \", counter[0])\n",
    "print(\"\")\n",
    "print(data[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing\n",
    "import string\n",
    "import re\n",
    "punc = string.punctuation+'\\n\\r\\t\"'\n",
    "def preprocess(text):\n",
    "    \"\"\"Suppressing numbers, lowering strings and removing punctuations\"\"\"\n",
    "    chiffsupp = re.sub('[0-9]+', '', text)\n",
    "    return chiffsupp.translate(str.maketrans(punc, ' ' * len(punc))).lower()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec: Quick Recap\n",
    "\n",
    "**[Word2Vec](https://arxiv.org/abs/1301.3781) is composed of two distinct language models (CBOW and SG), optimized to quickly learn word vectors**\n",
    "\n",
    "\n",
    "given a random text: `i'm taking the dog out for a walk`\n",
    "\n",
    "\n",
    "\n",
    "### (a) Continuous Bag of Word (CBOW)\n",
    "    -  predicts a word given a context\n",
    "    \n",
    "maximizing `p(dog | i'm taking the ___ out for a walk)`\n",
    "    \n",
    "### (b) Skip-Gram (SG)               \n",
    "    -  predicts a context given a word\n",
    "    \n",
    " maximizing `p(i'm taking the out for a walk | dog)`\n",
    "\n",
    "\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 1: train a language model (word2vec)\n",
    "\n",
    "Gensim has one of [Word2Vec](https://radimrehurek.com/gensim/models/word2vec.html) fastest implementation.\n",
    "\n",
    "\n",
    "### Train:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if gensim not installed yet\n",
    "# ! pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-27 01:09:21,151 : INFO : collecting all words and their counts\n",
      "2024-02-27 01:09:21,153 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2024-02-27 01:09:21,649 : INFO : PROGRESS: at sentence #10000, processed 2301366 words, keeping 153853 word types\n",
      "2024-02-27 01:09:22,147 : INFO : PROGRESS: at sentence #20000, processed 4553558 words, keeping 240043 word types\n",
      "2024-02-27 01:09:22,380 : INFO : collected 276678 word types from a corpus of 5713167 raw words and 25000 sentences\n",
      "2024-02-27 01:09:22,381 : INFO : Creating a fresh vocabulary\n",
      "2024-02-27 01:09:22,614 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 48208 unique words (17.42% of original 276678, drops 228470)', 'datetime': '2024-02-27T01:09:22.614486', 'gensim': '4.3.0', 'python': '3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'prepare_vocab'}\n",
      "2024-02-27 01:09:22,616 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 5389596 word corpus (94.34% of original 5713167, drops 323571)', 'datetime': '2024-02-27T01:09:22.616488', 'gensim': '4.3.0', 'python': '3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'prepare_vocab'}\n",
      "2024-02-27 01:09:22,890 : INFO : deleting the raw counts dictionary of 276678 items\n",
      "2024-02-27 01:09:22,899 : INFO : sample=0.001 downsamples 44 most-common words\n",
      "2024-02-27 01:09:22,900 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 4165010.0598832574 word corpus (77.3%% of prior 5389596)', 'datetime': '2024-02-27T01:09:22.900489', 'gensim': '4.3.0', 'python': '3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'prepare_vocab'}\n",
      "2024-02-27 01:09:23,296 : INFO : estimated required memory for 48208 words and 100 dimensions: 62670400 bytes\n",
      "2024-02-27 01:09:23,297 : INFO : resetting layer weights\n",
      "2024-02-27 01:09:23,325 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-02-27T01:09:23.325489', 'gensim': '4.3.0', 'python': '3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'build_vocab'}\n",
      "2024-02-27 01:09:23,327 : INFO : Word2Vec lifecycle event {'msg': 'training model with 3 workers on 48208 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2024-02-27T01:09:23.327495', 'gensim': '4.3.0', 'python': '3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'train'}\n",
      "2024-02-27 01:09:24,361 : INFO : EPOCH 0 - PROGRESS: at 5.73% examples, 235781 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 01:09:25,389 : INFO : EPOCH 0 - PROGRESS: at 12.51% examples, 253503 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 01:09:26,431 : INFO : EPOCH 0 - PROGRESS: at 19.18% examples, 257680 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 01:09:27,433 : INFO : EPOCH 0 - PROGRESS: at 24.94% examples, 253859 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 01:09:28,458 : INFO : EPOCH 0 - PROGRESS: at 31.02% examples, 251845 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 01:09:29,466 : INFO : EPOCH 0 - PROGRESS: at 37.03% examples, 252334 words/s, in_qsize 6, out_qsize 0\n",
      "2024-02-27 01:09:30,498 : INFO : EPOCH 0 - PROGRESS: at 43.43% examples, 252024 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 01:09:31,505 : INFO : EPOCH 0 - PROGRESS: at 49.61% examples, 252313 words/s, in_qsize 6, out_qsize 0\n",
      "2024-02-27 01:09:32,513 : INFO : EPOCH 0 - PROGRESS: at 55.73% examples, 251855 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 01:09:33,549 : INFO : EPOCH 0 - PROGRESS: at 61.83% examples, 251682 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 01:09:34,572 : INFO : EPOCH 0 - PROGRESS: at 67.44% examples, 249901 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 01:09:35,632 : INFO : EPOCH 0 - PROGRESS: at 73.28% examples, 247737 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 01:09:36,675 : INFO : EPOCH 0 - PROGRESS: at 78.90% examples, 245172 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 01:09:37,701 : INFO : EPOCH 0 - PROGRESS: at 84.29% examples, 243667 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 01:09:38,765 : INFO : EPOCH 0 - PROGRESS: at 90.14% examples, 242311 words/s, in_qsize 6, out_qsize 0\n",
      "2024-02-27 01:09:39,768 : INFO : EPOCH 0 - PROGRESS: at 95.08% examples, 240629 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 01:09:40,526 : INFO : EPOCH 0: training on 5713167 raw words (4164785 effective words) took 17.2s, 242224 effective words/s\n",
      "2024-02-27 01:09:41,580 : INFO : EPOCH 1 - PROGRESS: at 5.59% examples, 224543 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 01:09:42,628 : INFO : EPOCH 1 - PROGRESS: at 11.21% examples, 224749 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 01:09:43,641 : INFO : EPOCH 1 - PROGRESS: at 17.03% examples, 226907 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 01:09:44,644 : INFO : EPOCH 1 - PROGRESS: at 22.90% examples, 232201 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 01:09:45,662 : INFO : EPOCH 1 - PROGRESS: at 28.92% examples, 234962 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 01:09:46,669 : INFO : EPOCH 1 - PROGRESS: at 34.74% examples, 236001 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 01:09:47,700 : INFO : EPOCH 1 - PROGRESS: at 40.94% examples, 237949 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 01:09:48,713 : INFO : EPOCH 1 - PROGRESS: at 46.85% examples, 238219 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 01:09:49,717 : INFO : EPOCH 1 - PROGRESS: at 52.65% examples, 237717 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 01:09:50,720 : INFO : EPOCH 1 - PROGRESS: at 58.32% examples, 237531 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 01:09:51,729 : INFO : EPOCH 1 - PROGRESS: at 63.84% examples, 237339 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 01:09:52,743 : INFO : EPOCH 1 - PROGRESS: at 69.94% examples, 238363 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 01:09:53,752 : INFO : EPOCH 1 - PROGRESS: at 76.39% examples, 239822 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 01:09:54,809 : INFO : EPOCH 1 - PROGRESS: at 83.01% examples, 241233 words/s, in_qsize 6, out_qsize 0\n",
      "2024-02-27 01:09:55,823 : INFO : EPOCH 1 - PROGRESS: at 89.81% examples, 243647 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 01:09:56,827 : INFO : EPOCH 1 - PROGRESS: at 95.82% examples, 244484 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 01:09:57,561 : INFO : EPOCH 1: training on 5713167 raw words (4165042 effective words) took 17.0s, 244563 effective words/s\n",
      "2024-02-27 01:09:58,569 : INFO : EPOCH 2 - PROGRESS: at 5.73% examples, 241520 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 01:09:59,578 : INFO : EPOCH 2 - PROGRESS: at 11.92% examples, 248103 words/s, in_qsize 6, out_qsize 0\n",
      "2024-02-27 01:10:00,636 : INFO : EPOCH 2 - PROGRESS: at 18.53% examples, 250748 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 01:10:01,648 : INFO : EPOCH 2 - PROGRESS: at 24.80% examples, 253185 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 01:10:02,649 : INFO : EPOCH 2 - PROGRESS: at 31.00% examples, 253925 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 01:10:03,653 : INFO : EPOCH 2 - PROGRESS: at 36.88% examples, 253103 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 01:10:04,654 : INFO : EPOCH 2 - PROGRESS: at 43.25% examples, 253719 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 01:10:05,668 : INFO : EPOCH 2 - PROGRESS: at 49.46% examples, 253562 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 01:10:06,713 : INFO : EPOCH 2 - PROGRESS: at 55.55% examples, 251962 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 01:10:07,728 : INFO : EPOCH 2 - PROGRESS: at 61.67% examples, 252310 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 01:10:08,729 : INFO : EPOCH 2 - PROGRESS: at 67.98% examples, 253582 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 01:10:09,796 : INFO : EPOCH 2 - PROGRESS: at 74.78% examples, 253860 words/s, in_qsize 6, out_qsize 0\n",
      "2024-02-27 01:10:10,838 : INFO : EPOCH 2 - PROGRESS: at 81.16% examples, 253498 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 01:10:11,842 : INFO : EPOCH 2 - PROGRESS: at 87.33% examples, 253814 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 01:10:12,847 : INFO : EPOCH 2 - PROGRESS: at 93.16% examples, 253551 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 01:10:13,869 : INFO : EPOCH 2 - PROGRESS: at 99.17% examples, 253111 words/s, in_qsize 6, out_qsize 0\n",
      "2024-02-27 01:10:13,959 : INFO : EPOCH 2: training on 5713167 raw words (4163414 effective words) took 16.4s, 253973 effective words/s\n",
      "2024-02-27 01:10:15,012 : INFO : EPOCH 3 - PROGRESS: at 5.73% examples, 232092 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 01:10:16,037 : INFO : EPOCH 3 - PROGRESS: at 12.51% examples, 251838 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 01:10:17,060 : INFO : EPOCH 3 - PROGRESS: at 19.02% examples, 255928 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 01:10:18,097 : INFO : EPOCH 3 - PROGRESS: at 25.61% examples, 258934 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 01:10:19,131 : INFO : EPOCH 3 - PROGRESS: at 32.36% examples, 261012 words/s, in_qsize 6, out_qsize 0\n",
      "2024-02-27 01:10:20,173 : INFO : EPOCH 3 - PROGRESS: at 39.00% examples, 262084 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 01:10:21,208 : INFO : EPOCH 3 - PROGRESS: at 45.71% examples, 263201 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 01:10:22,233 : INFO : EPOCH 3 - PROGRESS: at 52.65% examples, 264161 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 01:10:23,248 : INFO : EPOCH 3 - PROGRESS: at 58.66% examples, 262287 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 01:10:24,249 : INFO : EPOCH 3 - PROGRESS: at 64.74% examples, 261918 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 01:10:25,257 : INFO : EPOCH 3 - PROGRESS: at 70.96% examples, 261532 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 01:10:26,259 : INFO : EPOCH 3 - PROGRESS: at 77.85% examples, 262548 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 01:10:27,280 : INFO : EPOCH 3 - PROGRESS: at 84.00% examples, 261881 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 01:10:28,321 : INFO : EPOCH 3 - PROGRESS: at 90.31% examples, 260945 words/s, in_qsize 6, out_qsize 0\n",
      "2024-02-27 01:10:29,329 : INFO : EPOCH 3 - PROGRESS: at 96.22% examples, 260195 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 01:10:30,009 : INFO : EPOCH 3: training on 5713167 raw words (4164110 effective words) took 16.0s, 259535 effective words/s\n",
      "2024-02-27 01:10:31,045 : INFO : EPOCH 4 - PROGRESS: at 5.73% examples, 235324 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 01:10:32,052 : INFO : EPOCH 4 - PROGRESS: at 11.92% examples, 245284 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 01:10:33,069 : INFO : EPOCH 4 - PROGRESS: at 18.01% examples, 244953 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 01:10:34,072 : INFO : EPOCH 4 - PROGRESS: at 23.46% examples, 240678 words/s, in_qsize 6, out_qsize 0\n",
      "2024-02-27 01:10:35,073 : INFO : EPOCH 4 - PROGRESS: at 29.65% examples, 243969 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 01:10:36,084 : INFO : EPOCH 4 - PROGRESS: at 35.40% examples, 243324 words/s, in_qsize 6, out_qsize 0\n",
      "2024-02-27 01:10:37,084 : INFO : EPOCH 4 - PROGRESS: at 42.16% examples, 248302 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 01:10:38,095 : INFO : EPOCH 4 - PROGRESS: at 48.38% examples, 249190 words/s, in_qsize 6, out_qsize 0\n",
      "2024-02-27 01:10:39,101 : INFO : EPOCH 4 - PROGRESS: at 54.88% examples, 250551 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 01:10:40,111 : INFO : EPOCH 4 - PROGRESS: at 60.96% examples, 251131 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 01:10:41,118 : INFO : EPOCH 4 - PROGRESS: at 67.07% examples, 251743 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 01:10:42,173 : INFO : EPOCH 4 - PROGRESS: at 73.28% examples, 250671 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 01:10:43,176 : INFO : EPOCH 4 - PROGRESS: at 79.80% examples, 251312 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 01:10:44,186 : INFO : EPOCH 4 - PROGRESS: at 85.31% examples, 250148 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 01:10:45,186 : INFO : EPOCH 4 - PROGRESS: at 91.36% examples, 250291 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 01:10:46,214 : INFO : EPOCH 4 - PROGRESS: at 97.46% examples, 250383 words/s, in_qsize 6, out_qsize 0\n",
      "2024-02-27 01:10:46,610 : INFO : EPOCH 4: training on 5713167 raw words (4165497 effective words) took 16.6s, 251002 effective words/s\n",
      "2024-02-27 01:10:46,611 : INFO : Word2Vec lifecycle event {'msg': 'training on 28565835 raw words (20822848 effective words) took 83.3s, 250026 effective words/s', 'datetime': '2024-02-27T01:10:46.611498', 'gensim': '4.3.0', 'python': '3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'train'}\n",
      "2024-02-27 01:10:46,614 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=48208, vector_size=100, alpha=0.025>', 'datetime': '2024-02-27T01:10:46.614491', 'gensim': '4.3.0', 'python': '3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "text = [t.split() for t,p in data]\n",
    "\n",
    "# the following configuration is the default configuration\n",
    "w2v = gensim.models.word2vec.Word2Vec(sentences=text,\n",
    "                                vector_size=100, window=5,               ### here we train a cbow model \n",
    "                                min_count=5,                      \n",
    "                                sample=0.001, workers=3,\n",
    "                                sg=1, hs=0, negative=5,        ### set sg to 1 to train a sg model\n",
    "                                cbow_mean=1, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-27 01:11:35,857 : INFO : Word2Vec lifecycle event {'fname_or_handle': 'W2v-movies.dat', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2024-02-27T01:11:35.857444', 'gensim': '4.3.0', 'python': '3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'saving'}\n",
      "2024-02-27 01:11:35,858 : INFO : not storing attribute cum_table\n",
      "2024-02-27 01:11:35,930 : INFO : saved W2v-movies.dat\n"
     ]
    }
   ],
   "source": [
    "# Worth it to save the previous embedding\n",
    "w2v.save(\"W2v-movies.dat\")\n",
    "# You will be able to reload them:\n",
    "# w2v = gensim.models.Word2Vec.load(\"W2v-movies.dat\")\n",
    "# and you can continue the learning process if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 2: Test learnt embeddings\n",
    "\n",
    "The word embedding space directly encodes similarities between words: the vector coding for the word \"great\" will be closer to the vector coding for \"good\" than to the one coding for \"bad\". Generally, [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity) is the distance used when considering distance between vectors.\n",
    "\n",
    "KeyedVectors have a built in [similarity](https://radimrehurek.com/gensim/models /keyedvectors.html#gensim.models.keyedvectors.BaseKeyedVectors.similarity) method to compute the cosine similarity between words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "great and good: 0.7843455\n",
      "great and bad: 0.47022957\n"
     ]
    }
   ],
   "source": [
    "# is great really closer to good than to bad ?\n",
    "print(\"great and good:\",w2v.wv.similarity(\"great\",\"good\"))\n",
    "print(\"great and bad:\",w2v.wv.similarity(\"great\",\"bad\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since cosine distance encodes similarity, neighboring words are supposed to be similar. The [most_similar](https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.BaseKeyedVectors.most_similar) method returns the `topn` words given a query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('film', 0.9289810061454773), ('\"movie\"', 0.8283028602600098), ('flick', 0.7667436599731445), ('movie,', 0.7451980710029602), ('\"film\"', 0.7428070902824402)]\n",
      "[('amazing', 0.7817275524139404), ('excellent', 0.7275443077087402), ('awesome,', 0.6990002989768982), ('exceptional', 0.6928319931030273), ('cool', 0.6804935336112976)]\n",
      "[('actor,', 0.8143014907836914), ('actor.', 0.7617161273956299), ('Reeves', 0.7540039420127869), ('Hopper', 0.7491146326065063), ('actress', 0.734898567199707)]\n"
     ]
    }
   ],
   "source": [
    "# The query can be as simple as a word, such as \"movie\"\n",
    "\n",
    "# Try changing the word\n",
    "print(w2v.wv.most_similar(\"movie\",topn=5)) # 5 most similar words\n",
    "print(w2v.wv.most_similar(\"awesome\",topn=5))\n",
    "print(w2v.wv.most_similar(\"actor\",topn=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But it can be a more complicated query\n",
    "Word embedding spaces tend to encode much more.\n",
    "\n",
    "The most famous exemple is: `vec(king) - vec(man) + vec(woman) => vec(queen)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('awful', 0.7694743871688843),\n",
       " ('horrible', 0.6754423975944519),\n",
       " ('atrocious', 0.6486415266990662)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What is awesome - good + bad ?\n",
    "w2v.wv.most_similar(positive=[\"awesome\",\"bad\"],negative=[\"good\"],topn=3)  \n",
    "\n",
    "\n",
    "# Try other things like plurals for exemple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('actress', 0.8380486965179443),\n",
       " ('actress,', 0.7827675342559814),\n",
       " ('actress.', 0.6776851415634155)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.wv.most_similar(positive=[\"actor\",\"woman\"],negative=[\"man\"],topn=3) # do the famous exemple works for actor ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('actresses', 0.765678882598877),\n",
       " ('actors/actresses', 0.6931259036064148),\n",
       " ('actors,', 0.6719549298286438)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.wv.most_similar(positive=[\"actors\",\"women\"],negative=[\"men\"],topn=3) # \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To test learnt \"synctactic\" and \"semantic\" similarities, Mikolov et al. introduced a special dataset containing a wide variety of three way similarities.**\n",
    "\n",
    "**You can download the dataset [here](https://thome.isir.upmc.fr/classes/RITAL/questions-words.txt).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-27 01:11:37,763 : INFO : Evaluating word analogies for top 300000 words in the model on ressources/questions-words.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-27 01:11:37,900 : INFO : capital-common-countries: 7.8% (7/90)\n",
      "2024-02-27 01:11:38,019 : INFO : capital-world: 2.8% (2/71)\n",
      "2024-02-27 01:11:38,062 : INFO : currency: 0.0% (0/28)\n",
      "2024-02-27 01:11:38,550 : INFO : city-in-state: 0.0% (0/329)\n",
      "2024-02-27 01:11:39,076 : INFO : family: 34.8% (119/342)\n",
      "2024-02-27 01:11:40,405 : INFO : gram1-adjective-to-adverb: 1.9% (18/930)\n",
      "2024-02-27 01:11:41,241 : INFO : gram2-opposite: 3.6% (20/552)\n",
      "2024-02-27 01:11:42,871 : INFO : gram3-comparative: 19.0% (240/1260)\n",
      "2024-02-27 01:11:43,803 : INFO : gram4-superlative: 7.0% (49/702)\n",
      "2024-02-27 01:11:44,815 : INFO : gram5-present-participle: 16.8% (127/756)\n",
      "2024-02-27 01:11:45,874 : INFO : gram6-nationality-adjective: 2.4% (19/792)\n",
      "2024-02-27 01:11:47,568 : INFO : gram7-past-tense: 17.0% (214/1260)\n",
      "2024-02-27 01:11:48,599 : INFO : gram8-plural: 4.9% (40/812)\n",
      "2024-02-27 01:11:49,808 : INFO : gram9-plural-verbs: 25.9% (196/756)\n",
      "2024-02-27 01:11:49,809 : INFO : Quadruplets with out-of-vocabulary words: 55.6%\n",
      "2024-02-27 01:11:49,810 : INFO : NB: analogies containing OOV words were skipped from evaluation! To change this behavior, use \"dummy4unknown=True\"\n",
      "2024-02-27 01:11:49,811 : INFO : Total accuracy: 12.1% (1051/8680)\n"
     ]
    }
   ],
   "source": [
    "out = w2v.wv.evaluate_word_analogies(\"ressources/questions-words.txt\",case_insensitive=True)  #original semantic syntactic dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**When training the w2v models on the review dataset, since it hasn't been learnt with a lot of data, it does not perform very well.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 3: Loading a pre-trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Gensim, embeddings are loaded and can be used via the [\"KeyedVectors\"](https://radimrehurek.com/gensim/models/keyedvectors.html) class\n",
    "\n",
    "> Since trained word vectors are independent from the way they were trained (Word2Vec, FastText, WordRank, VarEmbed etc), they can be represented by a standalone structure, as implemented in this module.\n",
    "\n",
    ">The structure is called “KeyedVectors” and is essentially a mapping between entities and vectors. Each entity is identified by its string id, so this is a mapping between {str => 1D numpy array}.\n",
    "\n",
    ">The entity typically corresponds to a word (so the mapping maps words to 1D vectors), but for some models, they key can also correspond to a document, a graph node etc. To generalize over different use-cases, this module calls the keys entities. Each entity is always represented by its string id, no matter whether the entity is a word, a document or a graph node.\n",
    "\n",
    "**You can download the pre-trained word embedding [HERE](https://thome.isir.upmc.fr/classes/RITAL/word2vec-google-news-300.dat) .**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-27 01:11:49,847 : INFO : adding document #0 to Dictionary<0 unique tokens: []>\n",
      "2024-02-27 01:11:49,848 : INFO : built Dictionary<12 unique tokens: ['computer', 'human', 'interface', 'response', 'survey']...> from 9 documents (total 29 corpus positions)\n",
      "2024-02-27 01:11:49,849 : INFO : Dictionary lifecycle event {'msg': \"built Dictionary<12 unique tokens: ['computer', 'human', 'interface', 'response', 'survey']...> from 9 documents (total 29 corpus positions)\", 'datetime': '2024-02-27T01:11:49.849481', 'gensim': '4.3.0', 'python': '3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-27 01:11:49,979 : INFO : loading KeyedVectors object from word2vec-google-news-300/word2vec-google-news-300.dat\n",
      "2024-02-27 01:11:51,738 : INFO : loading vectors from word2vec-google-news-300/word2vec-google-news-300.dat.vectors.npy with mmap=None\n",
      "2024-02-27 01:11:56,231 : INFO : KeyedVectors lifecycle event {'fname': 'word2vec-google-news-300/word2vec-google-news-300.dat', 'datetime': '2024-02-27T01:11:56.231264', 'gensim': '4.3.0', 'python': '3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'loaded'}\n"
     ]
    }
   ],
   "source": [
    "from gensim.test.utils import get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "import gensim.downloader as api\n",
    "bload = True\n",
    "fname = \"word2vec-google-news-300\"\n",
    "sdir = \"word2vec-google-news-300/\" # Change\n",
    "\n",
    "if(bload==True):\n",
    "    wv_pre_trained = KeyedVectors.load(sdir+fname+\".dat\")\n",
    "else:    \n",
    "    wv_pre_trained = api.load(fname)\n",
    "    wv_pre_trained.save(sdir+fname+\".dat\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Perform the \"synctactic\" and \"semantic\" evaluations again. Conclude on the pre-trained embeddings.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-27 01:19:18,164 : INFO : Evaluating word analogies for top 300000 words in the model on ressources/questions-words_pretrained.txt\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'ressources/questions-words_pretrained.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m out \u001b[38;5;241m=\u001b[39m wv_pre_trained\u001b[38;5;241m.\u001b[39mevaluate_word_analogies(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mressources/questions-words_pretrained.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m,case_insensitive\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\PC\\anaconda3\\Lib\\site-packages\\gensim\\models\\keyedvectors.py:1335\u001b[0m, in \u001b[0;36mKeyedVectors.evaluate_word_analogies\u001b[1;34m(self, analogies, restrict_vocab, case_insensitive, dummy4unknown, similarity_function)\u001b[0m\n\u001b[0;32m   1333\u001b[0m sections, section \u001b[38;5;241m=\u001b[39m [], \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1334\u001b[0m quadruplets_no \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1335\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m utils\u001b[38;5;241m.\u001b[39mopen(analogies, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fin:\n\u001b[0;32m   1336\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m line_no, line \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(fin):\n\u001b[0;32m   1337\u001b[0m         line \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mto_unicode(line)\n",
      "File \u001b[1;32mc:\\Users\\PC\\anaconda3\\Lib\\site-packages\\smart_open\\smart_open_lib.py:188\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(uri, mode, buffering, encoding, errors, newline, closefd, opener, ignore_ext, compression, transport_params)\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transport_params \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    186\u001b[0m     transport_params \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m--> 188\u001b[0m fobj \u001b[38;5;241m=\u001b[39m _shortcut_open(\n\u001b[0;32m    189\u001b[0m     uri,\n\u001b[0;32m    190\u001b[0m     mode,\n\u001b[0;32m    191\u001b[0m     compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[0;32m    192\u001b[0m     buffering\u001b[38;5;241m=\u001b[39mbuffering,\n\u001b[0;32m    193\u001b[0m     encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[0;32m    194\u001b[0m     errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    195\u001b[0m     newline\u001b[38;5;241m=\u001b[39mnewline,\n\u001b[0;32m    196\u001b[0m )\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    198\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fobj\n",
      "File \u001b[1;32mc:\\Users\\PC\\anaconda3\\Lib\\site-packages\\smart_open\\smart_open_lib.py:361\u001b[0m, in \u001b[0;36m_shortcut_open\u001b[1;34m(uri, mode, compression, buffering, encoding, errors, newline)\u001b[0m\n\u001b[0;32m    358\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m    359\u001b[0m     open_kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124merrors\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m errors\n\u001b[1;32m--> 361\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _builtin_open(local_path, mode, buffering\u001b[38;5;241m=\u001b[39mbuffering, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mopen_kwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'ressources/questions-words_pretrained.txt'"
     ]
    }
   ],
   "source": [
    "out = wv_pre_trained.evaluate_word_analogies(\"ressources/questions-words_pretrained.txt\",case_insensitive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "great and good: 0.72915095\n",
      "great and bad: 0.3928765\n"
     ]
    }
   ],
   "source": [
    "print(\"great and good:\",wv_pre_trained.similarity(\"great\",\"good\"))\n",
    "print(\"great and bad:\",wv_pre_trained.similarity(\"great\",\"bad\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "movie :\n",
      "\t ('film', 0.8676770329475403)\n",
      "\t ('movies', 0.8013108372688293)\n",
      "\t ('films', 0.7363011837005615)\n",
      "\t ('moive', 0.6830360889434814)\n",
      "\t ('Movie', 0.6693680286407471)\n",
      "awesome :\n",
      "\t ('amazing', 0.8282866477966309)\n",
      "\t ('unbelievable', 0.74649578332901)\n",
      "\t ('fantastic', 0.7453290224075317)\n",
      "\t ('incredible', 0.7390913367271423)\n",
      "\t ('unbelieveable', 0.6678116917610168)\n",
      "actor :\n",
      "\t ('actress', 0.7930010557174683)\n",
      "\t ('Actor', 0.7446156740188599)\n",
      "\t ('thesp', 0.6954971551895142)\n",
      "\t ('thespian', 0.6651668548583984)\n",
      "\t ('actors', 0.6519852876663208)\n"
     ]
    }
   ],
   "source": [
    "for word in ['movie', 'awesome', 'actor']:\n",
    "    print(word, ':')\n",
    "    for item in wv_pre_trained.most_similar(word,topn=5):\n",
    "        print('\\t', item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "awesome bad -- good\n",
      "\t ('horrible', 0.5953484177589417)\n",
      "\t ('amazing', 0.5928210020065308)\n",
      "\t ('weird', 0.5782381296157837)\n",
      "\t ('freaky', 0.5767403244972229)\n",
      "\t ('unbelievable', 0.5747914910316467)\n",
      "actor woman -- man\n",
      "\t ('actress', 0.8602624535560608)\n",
      "\t ('actresses', 0.6596670150756836)\n",
      "\t ('thesp', 0.6290916800498962)\n",
      "\t ('Actress', 0.6165294647216797)\n",
      "\t ('actress_Rachel_Weisz', 0.5997323989868164)\n"
     ]
    }
   ],
   "source": [
    "positives = [\n",
    "    [\"awesome\",\"bad\"],\n",
    "    [\"actor\",\"woman\"]\n",
    "]\n",
    "\n",
    "negatives = [\n",
    "    [\"good\"],\n",
    "    [\"man\"]\n",
    "]\n",
    "\n",
    "for i in range(2):\n",
    "    print(*positives[i], '--', *negatives[i])\n",
    "    for item in wv_pre_trained.most_similar(positive=positives[i],negative=negatives[i],topn=5):\n",
    "        print('\\t', item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "movie movies -- film\n",
      "\t ('films', 0.6389263868331909)\n",
      "\t ('Movies', 0.6188486218452454)\n",
      "\t ('flicks', 0.6120561361312866)\n",
      "\t ('Hollywood_blockbusters', 0.5958892703056335)\n",
      "\t ('romcoms', 0.5864962339401245)\n",
      "actor actors -- actress\n",
      "\t ('Actors', 0.6128960251808167)\n",
      "\t ('thesps', 0.5747196078300476)\n",
      "\t ('screenwriters', 0.5588046312332153)\n",
      "\t ('thespians', 0.5531652569770813)\n",
      "\t ('thespian', 0.5476460456848145)\n"
     ]
    }
   ],
   "source": [
    "positives = [\n",
    "    [\"movie\",\"movies\"],\n",
    "    [\"actor\",\"actors\"]\n",
    "]\n",
    "\n",
    "negatives = [\n",
    "    [\"film\"],\n",
    "    [\"actress\"]\n",
    "]\n",
    "\n",
    "for i in range(2):\n",
    "    print(*positives[i], '--', *negatives[i])\n",
    "    for item in wv_pre_trained.most_similar(positive=positives[i],negative=negatives[i],topn=5):\n",
    "        print('\\t', item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 4:  sentiment classification\n",
    "\n",
    "In the previous practical session, we used a bag of word approach to transform text into vectors.\n",
    "Here, we propose to try to use word vectors (previously learnt or loaded).\n",
    "\n",
    "\n",
    "### <font color='green'> Since we have only word vectors and that sentences are made of multiple words, we need to aggregate them. </font>\n",
    "\n",
    "\n",
    "### (1) Vectorize reviews using word vectors:\n",
    "\n",
    "Word aggregation can be done in different ways:\n",
    "\n",
    "- Sum\n",
    "- Average\n",
    "- Min/feature\n",
    "- Max/feature\n",
    "\n",
    "#### a few pointers:\n",
    "\n",
    "- `w2v.wv.vocab` is a `set()` of the vocabulary (all existing words in your model)\n",
    "- `np.minimum(a,b) and np.maximum(a,b)` respectively return element-wise min/max "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  -2.37627046   32.90345695  -22.36017699   -1.88356059   11.65108255\n",
      " -106.57608645    4.58677254  179.66670126  -67.97166419  -76.83892138\n",
      "   16.09992365 -100.60202609    2.30489281   84.6385065   -15.19924983\n",
      "  -25.08711765   19.02260561  -60.92727518  -21.38774537 -221.77669695\n",
      "   53.83795181    9.30805586  158.35142657  -91.08885736  -18.84795691\n",
      "   77.33316899  -47.85235558   20.23314666 -104.13862743   81.14041466\n",
      "   65.81978434   17.8525585    27.8722761  -138.68033748  -34.42874938\n",
      "   63.72472595   50.04359605  -77.72665762  -67.26169342 -157.09248285\n",
      "   -6.43196663 -160.29360995  -62.9862789    44.3848597   104.91261487\n",
      "  -43.62902318  -76.69970658   20.4128542    14.89370537   14.50099317\n",
      "   65.89867352 -100.22173624   53.86226202  -36.92350398  -40.82686786\n",
      "    9.17822228   -8.21536259   42.3206258   -44.35313587   53.63165555\n",
      "   41.9501717   -25.40144322   22.63124714   72.12487827 -104.50473\n",
      "  140.31925851   -5.23539437   73.04332452  -89.99325113   87.20401604\n",
      "   -7.39640753  102.41129418   59.09568765    3.21218251   90.344581\n",
      "   23.21264828    6.73025867   11.9509456   -26.67139966   50.15621191\n",
      "  -57.69516997    4.68036027  -92.54807079  116.59551359  -29.57679054\n",
      "    3.28567961   66.03464622   53.49506293   97.6069826     8.04007246\n",
      "  132.62882845   76.01355787   20.41778702  -15.35634177  162.47645843\n",
      "   -6.23348386  106.73414712 -108.90987112    4.09625696  -18.73726573]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# We first need to vectorize text:\n",
    "# First we propose to a sum of them\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def randomvec():\n",
    "    default = np.random.randn(100)\n",
    "    default = default  / np.linalg.norm(default)\n",
    "    return default\n",
    "\n",
    "def vectorize(text,func=np.sum):\n",
    "    \"\"\"\n",
    "    This function should vectorize one review\n",
    "\n",
    "    input: str\n",
    "    output: np.array(float)\n",
    "    \"\"\"    \n",
    "\n",
    "    #for word in text:\n",
    "        # do something\n",
    "    vec = []\n",
    "    for word in text:\n",
    "        if not (word in w2v.wv):\n",
    "            vec.append(randomvec())\n",
    "        else:\n",
    "            vec.append(w2v.wv[word])\n",
    "    #if mean:\n",
    "    #    return np.mean(vec,axis=0)\n",
    "    return func(np.array(vec),axis=0)\n",
    "    \n",
    "lab = [l for t,l in data]\n",
    "train,test, y_train, y_test = train_test_split(text,lab,test_size=0.2,random_state=42)\n",
    "#classes = [pol for text,pol in train]\n",
    "X_train = [vectorize(text) for text in train]\n",
    "X_test = [vectorize(text) for text in test]\n",
    "#true = [pol for text,pol in test]\n",
    "\n",
    "\n",
    "#let's see what a review vector looks like.\n",
    "print(X_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Train a classifier \n",
    "as in the previous practical session, train a logistic regression to do sentiment classification with word vectors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression()\n",
      "score sum= 0.8232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PC\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "train,test, y_train, y_test = train_test_split(text,lab,test_size=0.2,random_state=42)\n",
    "\n",
    "X_train = [vectorize(text,func=np.sum) for text in train]\n",
    "X_test = [vectorize(text,func=np.sum) for text in test]\n",
    "\n",
    "# Scikit Logistic Regression\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train,  y_train)  \n",
    "score = clf.score(X_test,y_test)\n",
    "print(clf)\n",
    "print(\"score sum=\",score)\n",
    "#print(\"classifier:\",clf.coef_) # retrieve the coefs from inside the object (cf doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression()\n",
      "score mean= 0.8148\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_train = [vectorize(text,func=np.mean) for text in train]\n",
    "X_test = [vectorize(text,func=np.mean) for text in test]\n",
    "clf1 = LogisticRegression()\n",
    "clf1.fit(X_train,  y_train)  \n",
    "score = clf1.score(X_test,y_test)\n",
    "print(clf1)\n",
    "print(\"score mean=\",score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression()\n",
      "score max= 0.6998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PC\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "X_train = [vectorize(text,func=np.max) for text in train]\n",
    "X_test = [vectorize(text,func=np.max) for text in test]\n",
    "clf2 = LogisticRegression()\n",
    "clf2.fit(X_train,  y_train)  \n",
    "score = clf2.score(X_test,y_test)\n",
    "print(clf2)\n",
    "print(\"score max=\",score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression()\n",
      "score min= 0.698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PC\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "X_train = [vectorize(text,func=np.min) for text in train]\n",
    "X_test = [vectorize(text,func=np.min) for text in test]\n",
    "clf3 = LogisticRegression()\n",
    "clf3.fit(X_train,  y_train)  \n",
    "score = clf3.score(X_test,y_test)\n",
    "print(clf3)\n",
    "print(\"score min=\",score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "performance should be worst than with bag of word (~80%). Sum/Mean aggregation does not work well on long reviews (especially with many frequent words). This adds a lot of noise.\n",
    "\n",
    "## **Todo**:  Try answering the following questions:\n",
    "\n",
    "- Which word2vec model works best: skip-gram or cbow\n",
    "- Do pretrained vectors work best than those learnt on the train dataset ?\n",
    "\n",
    "## **Todo**: evaluate the same pipeline on speaker ID task (Chirac/Mitterrand) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-27 01:18:06,060 : INFO : collecting all words and their counts\n",
      "2024-02-27 01:18:06,061 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2024-02-27 01:18:06,597 : INFO : PROGRESS: at sentence #10000, processed 2301366 words, keeping 153853 word types\n",
      "2024-02-27 01:18:07,064 : INFO : PROGRESS: at sentence #20000, processed 4553558 words, keeping 240043 word types\n",
      "2024-02-27 01:18:07,306 : INFO : collected 276678 word types from a corpus of 5713167 raw words and 25000 sentences\n",
      "2024-02-27 01:18:07,307 : INFO : Creating a fresh vocabulary\n",
      "2024-02-27 01:18:07,526 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 48208 unique words (17.42% of original 276678, drops 228470)', 'datetime': '2024-02-27T01:18:07.526104', 'gensim': '4.3.0', 'python': '3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'prepare_vocab'}\n",
      "2024-02-27 01:18:07,528 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 5389596 word corpus (94.34% of original 5713167, drops 323571)', 'datetime': '2024-02-27T01:18:07.528110', 'gensim': '4.3.0', 'python': '3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'prepare_vocab'}\n",
      "2024-02-27 01:18:07,777 : INFO : deleting the raw counts dictionary of 276678 items\n",
      "2024-02-27 01:18:07,783 : INFO : sample=0.001 downsamples 44 most-common words\n",
      "2024-02-27 01:18:07,784 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 4165010.0598832574 word corpus (77.3%% of prior 5389596)', 'datetime': '2024-02-27T01:18:07.784113', 'gensim': '4.3.0', 'python': '3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'prepare_vocab'}\n",
      "2024-02-27 01:18:08,148 : INFO : estimated required memory for 48208 words and 100 dimensions: 62670400 bytes\n",
      "2024-02-27 01:18:08,149 : INFO : resetting layer weights\n",
      "2024-02-27 01:18:08,172 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-02-27T01:18:08.172112', 'gensim': '4.3.0', 'python': '3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'build_vocab'}\n",
      "2024-02-27 01:18:08,175 : INFO : Word2Vec lifecycle event {'msg': 'training model with 3 workers on 48208 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2024-02-27T01:18:08.175111', 'gensim': '4.3.0', 'python': '3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'train'}\n",
      "2024-02-27 01:18:09,186 : INFO : EPOCH 0 - PROGRESS: at 23.27% examples, 963699 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 01:18:10,190 : INFO : EPOCH 0 - PROGRESS: at 45.91% examples, 951718 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 01:18:11,195 : INFO : EPOCH 0 - PROGRESS: at 69.26% examples, 955723 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 01:18:12,198 : INFO : EPOCH 0 - PROGRESS: at 90.72% examples, 938040 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 01:18:12,625 : INFO : EPOCH 0: training on 5713167 raw words (4165320 effective words) took 4.4s, 937085 effective words/s\n",
      "2024-02-27 01:18:13,637 : INFO : EPOCH 1 - PROGRESS: at 22.41% examples, 927484 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 01:18:14,639 : INFO : EPOCH 1 - PROGRESS: at 44.06% examples, 913338 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 01:18:15,640 : INFO : EPOCH 1 - PROGRESS: at 64.18% examples, 887709 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 01:18:16,643 : INFO : EPOCH 1 - PROGRESS: at 86.44% examples, 893973 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 01:18:17,219 : INFO : EPOCH 1: training on 5713167 raw words (4164569 effective words) took 4.6s, 907528 effective words/s\n",
      "2024-02-27 01:18:18,228 : INFO : EPOCH 2 - PROGRESS: at 21.07% examples, 872239 words/s, in_qsize 6, out_qsize 0\n",
      "2024-02-27 01:18:19,230 : INFO : EPOCH 2 - PROGRESS: at 41.61% examples, 864096 words/s, in_qsize 6, out_qsize 0\n",
      "2024-02-27 01:18:20,232 : INFO : EPOCH 2 - PROGRESS: at 62.14% examples, 859537 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 01:18:21,241 : INFO : EPOCH 2 - PROGRESS: at 83.69% examples, 864743 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 01:18:22,007 : INFO : EPOCH 2: training on 5713167 raw words (4164794 effective words) took 4.8s, 870677 effective words/s\n",
      "2024-02-27 01:18:23,019 : INFO : EPOCH 3 - PROGRESS: at 21.88% examples, 906433 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 01:18:24,023 : INFO : EPOCH 3 - PROGRESS: at 44.92% examples, 930459 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 01:18:25,023 : INFO : EPOCH 3 - PROGRESS: at 67.23% examples, 930911 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 01:18:26,031 : INFO : EPOCH 3 - PROGRESS: at 90.56% examples, 936078 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 01:18:26,462 : INFO : EPOCH 3: training on 5713167 raw words (4165634 effective words) took 4.4s, 936152 effective words/s\n",
      "2024-02-27 01:18:27,475 : INFO : EPOCH 4 - PROGRESS: at 21.23% examples, 876054 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 01:18:28,482 : INFO : EPOCH 4 - PROGRESS: at 42.38% examples, 874487 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 01:18:29,483 : INFO : EPOCH 4 - PROGRESS: at 62.99% examples, 868779 words/s, in_qsize 4, out_qsize 1\n",
      "2024-02-27 01:18:30,489 : INFO : EPOCH 4 - PROGRESS: at 85.03% examples, 877390 words/s, in_qsize 5, out_qsize 0\n",
      "2024-02-27 01:18:31,215 : INFO : EPOCH 4: training on 5713167 raw words (4163725 effective words) took 4.7s, 876789 effective words/s\n",
      "2024-02-27 01:18:31,216 : INFO : Word2Vec lifecycle event {'msg': 'training on 28565835 raw words (20824042 effective words) took 23.0s, 903779 effective words/s', 'datetime': '2024-02-27T01:18:31.216782', 'gensim': '4.3.0', 'python': '3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'train'}\n",
      "2024-02-27 01:18:31,217 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=48208, vector_size=100, alpha=0.025>', 'datetime': '2024-02-27T01:18:31.217791', 'gensim': '4.3.0', 'python': '3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "#- Which word2vec model works best: skip-gram or cbow\n",
    "skipgram = w2v\n",
    "cbow =gensim.models.word2vec.Word2Vec(sentences=text,\n",
    "                                vector_size=100, window=5,               ### here we train a cbow model \n",
    "                                min_count=5,                      \n",
    "                                sample=0.001, workers=3,\n",
    "                                sg=0, hs=0, negative=5,        ### set sg to 1 to train a sg model\n",
    "                                cbow_mean=1, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S [('film', 0.9289810061454773), ('\"movie\"', 0.8283028602600098), ('flick', 0.7667436599731445), ('movie,', 0.7451980710029602), ('\"film\"', 0.7428070902824402)]\n",
      "C [('film', 0.9295904040336609), ('movie,', 0.8304710388183594), ('film,', 0.7681915163993835), ('flick', 0.747571587562561), ('documentary', 0.7352034449577332)]\n",
      "S [('amazing', 0.7817275524139404), ('excellent', 0.7275443077087402), ('awesome,', 0.6990002989768982), ('exceptional', 0.6928319931030273), ('cool', 0.6804935336112976)]\n",
      "C [('amazing', 0.8534489870071411), ('excellent', 0.8053512573242188), ('exceptional', 0.7795264720916748), ('incredible', 0.779024600982666), ('outstanding', 0.7754307985305786)]\n",
      "S [('actor,', 0.8143014907836914), ('actor.', 0.7617161273956299), ('Reeves', 0.7540039420127869), ('Hopper', 0.7491146326065063), ('actress', 0.734898567199707)]\n",
      "C [('actress', 0.8344793319702148), ('actor,', 0.8023119568824768), ('role', 0.7676254510879517), ('role,', 0.7433274984359741), ('performance', 0.7101996541023254)]\n"
     ]
    }
   ],
   "source": [
    "print(\"S\",skipgram.wv.most_similar(\"movie\",topn=5)) # 5 most similar words\n",
    "print(\"C\",cbow.wv.most_similar(\"movie\",topn=5)) # 5 most similar words\n",
    "print(\"S\",skipgram.wv.most_similar(\"awesome\",topn=5))\n",
    "print(\"C\",cbow.wv.most_similar(\"awesome\",topn=5))\n",
    "print(\"S\",skipgram.wv.most_similar(\"actor\",topn=5))\n",
    "print(\"C\",cbow.wv.most_similar(\"actor\",topn=5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cbow marche mieux avec un peu de pretraitement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**(Bonus)** To have a better accuracy, we could try two things:\n",
    "- Better aggregation methods (weight by tf-idf ?)\n",
    "- Another word vectorizing method such as [fasttext](https://radimrehurek.com/gensim/models/fasttext.html)\n",
    "- A document vectorizing method such as [Doc2Vec](https://radimrehurek.com/gensim/models/doc2vec.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "902a52bcf4503a473db011f1937bdfe17613b08622219712e0110e48c4958c23"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
